{ "cells": [
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Predição de Sucesso de Startups\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Este notebook documenta o processo de desenvolvimento de um modelo preditivo para identificar o sucesso ou insucesso de startups, com base em um conjunto de dados fornecido. O objetivo é seguir as etapas de um projeto de Machine Learning, desde a exploração dos dados até a avaliação e otimização do modelo, atendendo aos critérios de avaliação estabelecidos."
   ,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descrição do Dataset\n",
    "\n",
    "O dataset contém informações reais sobre startups, incluindo histórico de rodadas de investimento, valores captados, localização e áreas de atuação. A variável alvo `labels` indica o sucesso (1) ou insucesso (0) da startup. O dataset possui 923 linhas e 32 colunas, com algumas variáveis contendo valores ausentes (NaN) que necessitam de tratamento.\n",
    "\n",
    "### Variáveis Chave:\n",
    "\n",
    "*   **labels**: Variável alvo (0 = insucesso, 1 = sucesso).\n",
    "*   **age_first_funding_year, age_last_funding_year, age_first_milestone_year, age_last_milestone_year**: Idades relativas em anos.\n",
    "*   **relationships**: Contagem de relações (fundadores, executivos, investidores).\n",
    "*   **funding_rounds**: Número de rodadas de captação.\n",
    "*   **funding_total_usd**: Total captado em USD.\n",
    "*   **milestones**: Contagem de marcos relevantes.\n",
    "*   **avg_participants**: Média de investidores por rodada.\n",
    "*   **is_CA, is_NY, is_MA, is_TX, is_otherstate**: Dummies de localização.\n",
    "*   **category_code**: Setor principal (categórica).\n",
    "*   **is_software, is_web, is_mobile, is_enterprise, etc.**: Dummies de setor.\n",
    "*   **has_VC, has_angel, has_roundA, has_roundB, has_roundC, has_roundD**: Sinalizadores de financiamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Importações e Carregamento de Dados\n",
    "\n",
    "Para iniciar o projeto, são necessárias as importações das bibliotecas essenciais para manipulação de dados, pré-processamento, modelagem e avaliação. Em um ambiente Google Colab, a instalação de algumas bibliotecas pode ser necessária.\n",
    "\n",
    "```python\n",
    "# Instalação de bibliotecas (se necessário no Colab)\n",
    "!pip install category_encoders imbalanced-learn shap lightgbm catboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports para ML\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Balanceamento de classes avançado\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, classification_report, \n",
    "    confusion_matrix, precision_recall_curve, roc_curve, \n",
    "    average_precision_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "\n",
    "# Carregamento dos datasets (assumindo que os arquivos .csv estão no diretório /content/data/ ou no mesmo diretório do notebook)\n",
    "# Se estiver no Colab, você pode precisar montar o Google Drive ou fazer upload dos arquivos.\n",
    "# Exemplo para Colab: from google.colab import files; uploaded = files.upload()\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas e informações básicas\n",
    "print('Dados de Treino:')\n",
    "print(train.head())\n",
    "print('\\nInformações do Treino:')\n",
    "print(train.info())\n",
    "print('\\nDados de Teste:')\n",
    "print(test.head())\n",
    "print('\\nInformações do Teste:')\n",
    "print(test.info())\n",
    "```"
   ]
  }
]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descrição do Dataset\n",
    "\n",
    "O dataset contém informações reais sobre startups, incluindo histórico de rodadas de investimento, valores captados, localização e áreas de atuação. A variável alvo `labels` indica o sucesso (1) ou insucesso (0) da startup. O dataset possui 923 linhas e 32 colunas, com algumas variáveis contendo valores ausentes (NaN) que necessitam de tratamento.\n",
    "\n",
    "### Variáveis Chave:\n",
    "\n",
    "*   **labels**: Variável alvo (0 = insucesso, 1 = sucesso).\n",
    "*   **age_first_funding_year, age_last_funding_year, age_first_milestone_year, age_last_milestone_year**: Idades relativas em anos.\n",
    "*   **relationships**: Contagem de relações (fundadores, executivos, investidores).\n",
    "*   **funding_rounds**: Número de rodadas de captação.\n",
    "*   **funding_total_usd**: Total captado em USD.\n",
    "*   **milestones**: Contagem de marcos relevantes.\n",
    "*   **avg_participants**: Média de investidores por rodada.\n",
    "*   **is_CA, is_NY, is_MA, is_TX, is_otherstate**: Dummies de localização.\n",
    "*   **category_code**: Setor principal (categórica).\n",
    "*   **is_software, is_web, is_mobile, is_enterprise, etc.**: Dummies de setor.\n",
    "*   **has_VC, has_angel, has_roundA, has_roundB, has_roundC, has_roundD**: Sinalizadores de financiamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Importações e Carregamento de Dados\n",
    "\n",
    "Para iniciar o projeto, são necessárias as importações das bibliotecas essenciais para manipulação de dados, pré-processamento, modelagem e avaliação. Em um ambiente Google Colab, a instalação de algumas bibliotecas pode ser necessária.\n",
    "\n",
    "```python\n",
    "# Instalação de bibliotecas (se necessário no Colab)\n",
    "!pip install category_encoders imbalanced-learn shap lightgbm catboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports para ML\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Balanceamento de classes avançado\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, classification_report, \n",
    "    confusion_matrix, precision_recall_curve, roc_curve, \n",
    "    average_precision_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "\n",
    "# Carregamento dos datasets (assumindo que os arquivos .csv estão no diretório /content/data/ ou no mesmo diretório do notebook)\n",
    "# Se estiver no Colab, você pode precisar montar o Google Drive ou fazer upload dos arquivos.\n",
    "# Exemplo para Colab: from google.colab import files; uploaded = files.upload()\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas e informações básicas\n",
    "print('Dados de Treino:')\n",
    "print(train.head())\n",
    "print('\\nInformações do Treino:')\n",
    "print(train.info())\n",
    "print('\\nDados de Teste:')\n",
    "print(test.head())\n",
    "print('\\nInformações do Teste:')\n",
    "print(test.info())\n",
    "```"
   ]
  }
], "metadata": {}, "nbformat": 4, "nbformat_minor": 4 }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza e Tratamento de Valores Nulos\n",
    "\n",
    "A limpeza e o tratamento de valores nulos e *outliers* são etapas cruciais para garantir a qualidade dos dados e a robustez do modelo. O código base (`05.py` corrigido) implementa uma estratégia avançada para lidar com esses aspectos.\n",
    "\n",
    "### 3.1. Tratamento de Valores Ausentes (NaN)\n",
    "\n",
    "A função `advanced_preprocessing_pipeline` trata os valores ausentes de forma inteligente:\n",
    "\n",
    "*   **Features Temporais**: Para colunas como `age_first_funding_year`, `age_last_funding_year`, `age_first_milestone_year`, `age_last_milestone_year`, os valores `NaN` são interpretados como o evento não tendo ocorrido. Durante a engenharia de features, esses `NaN`s são preenchidos com valores altos (e.g., `999` ou `10`) para indicar a ausência do evento, o que é uma abordagem contextualizada.\n",
    "*   **Outras Features Numéricas**: Para as demais features numéricas, a imputação é feita utilizando a **mediana** (`SimpleImputer(strategy=\'median\')`). A mediana é preferível à média em distribuições assimétricas, pois é menos sensível a *outliers*. Embora o código original do `05.py` mencionasse `KNNImputer` nas importações, a implementação final na função `advanced_preprocessing_pipeline` optou por uma imputação mais direta com a mediana, o que é uma escolha válida para manter a simplicidade e robustez.\n",
    "\n",
    "```python\n",
    "# Trecho da função advanced_preprocessing_pipeline para tratamento de missing values\n",
    "def advanced_preprocessing_pipeline(X_train, y_train, X_test):\n",
    "    print(\"=== INICIANDO PIPELINE AVANÇADO ===\")\n",
    "    print(\"1. Tratamento de missing values...\")\n",
    "    \n",
    "    temporal_features = [\\'age_first_funding_year\\', \\'age_last_funding_year\\', \n",
    "                        \\'age_first_milestone_year\\', \\'age_last_milestone_year\\']\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        if col in temporal_features:\n",
    "            # Para temporais, manter NaN como valor alto (evento não ocorreu) - já tratado na FE\n",
    "            continue\n",
    "        elif X_train[col].dtype in [np.float64, np.int64]:\n",
    "            # Para outras numéricas, usar mediana\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "    return X_train, X_test\n",
    "```\n",
    "\n",
    "### 3.2. Tratamento de Outliers\n",
    "\n",
    "O tratamento de *outliers* é realizado de forma mais inteligente e seletiva:\n",
    "\n",
    "*   **Preservação de Outliers Informativos**: Features como `funding_total_usd`, `relationships` e `milestones` são consideradas críticas e seus *outliers* são preservados, pois valores extremos nessas variáveis podem ser fortes indicadores de sucesso ou insucesso de uma startup.\n",
    "*   **Clipping Conservador**: Para as demais features numéricas, o *clipping* é aplicado nos percentis de 5% e 95%. Esta abordagem é mais conservadora do que o IQR (25%-75%) e ajuda a mitigar o impacto de *outliers* extremos sem distorcer excessivamente a distribuição dos dados, sendo adequada para modelos baseados em árvores que são menos sensíveis a *outliers*.\n",
    "\n",
    "```python\n",
    "# Trecho da função advanced_preprocessing_pipeline para tratamento de outliers\n",
    "    print(\"2. Tratamento de outliers...\")\n",
    "    \n",
    "    preserve_outliers = [\\'funding_total_usd\\', \\'relationships\\', \\'milestones\\']\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].dtype in [np.float64, np.int64] and col not in preserve_outliers:\n",
    "            Q1 = X_train[col].quantile(0.05)\n",
    "            Q3 = X_train[col].quantile(0.95)\n",
    "            X_train[col] = X_train[col].clip(Q1, Q3)\n",
    "            X_test[col] = X_test[col].clip(Q1, Q3)\n",
    "    \n",
    "    print(\"3. Pipeline de preprocessing concluído!\")\n",
    "    return X_train, X_test\n",
    "```\n",
    "\n",
    "### 3.3. Feature Scaling\n",
    "\n",
    "O pipeline final utiliza `RobustScaler` para o escalonamento das features. Este escalonador é robusto a *outliers*, pois utiliza a mediana e o intervalo interquartil para escalar os dados, sendo uma escolha apropriada após o tratamento seletivo de *outliers*. Além disso, a engenharia de features já inclui transformações logarítmicas em variáveis de funding para reduzir a assimetria.\n",
    "\n",
    "```python\n",
    "# Trecho da função create_advanced_ml_pipeline\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "      (\\'scaler\\', scaler),\n",
    "      # ... outras etapas ...\n",
    "    ])\n",
    "```\n"
   ]
  }
]

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Codificação de Variáveis Categóricas\n",
    "\n",
    "A codificação de variáveis categóricas é essencial para transformar dados não numéricos em um formato que os modelos de *machine learning* possam processar. No pipeline final, a variável `category_code` não é explicitamente tratada por `OneHotEncoder` ou `TargetEncoder` no `advanced_preprocessing_pipeline`. Isso sugere que a estratégia adotada foi a de confiar nas variáveis *dummy* de setor (`is_software`, `is_web`, etc.) já existentes no dataset, que representam categorias mais amplas, e nas novas *features* engenheiradas que capturam informações setoriais e geográficas.\n",
    "\n",
    "As *features* binárias (`is_CA`, `has_VC`, etc.) são passadas diretamente para o modelo, pois já estão no formato numérico adequado (0 ou 1). As novas *features* criadas na engenharia de features, como `is_innovation_hub` ou `is_tech_sector`, também são binárias ou numéricas, eliminando a necessidade de um passo de codificação separado para `category_code` no pipeline principal.\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploração e Visualização dos Dados\n",
    "\n",
    "A Análise Exploratória de Dados (EDA) e a visualização são etapas fundamentais para compreender a estrutura dos dados, identificar padrões, anomalias e guiar as decisões de engenharia de *features* e seleção de modelos. Embora o código base (`05.py` corrigido) não inclua diretamente as visualizações, ele é construído sobre os *insights* obtidos em etapas exploratórias anteriores.\n",
    "\n",
    "### 5.1. Distribuição da Variável Alvo\n",
    "\n",
    "É crucial entender o balanceamento da variável alvo (`labels`). O dataset, conforme descrito, é moderadamente desbalanceado (aproximadamente 64.7% de sucesso e 35.3% de insucesso). A visualização da distribuição do *target* é o primeiro passo para identificar a necessidade de técnicas de balanceamento de classes.\n",
    "\n",
    "```python\n",
    "# Exemplo de código para visualizar a distribuição do target\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=train[\\'labels\\'])\n",
    "plt.title(\\'Distribuição dos Labels (Sucesso vs Fracasso)\\')\n",
    "plt.xlabel(\\'Label (0: Fracasso, 1: Sucesso)\\')\n",
    "plt.ylabel(\\'Quantidade\\')\n",
    "plt.show()\n",
    "\n",
    "print(\\'Distribuição do target (labels):\\')\n",
    "print(train[\\'labels\\'].value_counts(normalize=True))\n",
    "```\n",
    "\n",
    "### 5.2. Estatísticas Descritivas e Correlações\n",
    "\n",
    "Um resumo estatístico das *features* contínuas fornece uma visão rápida sobre a distribuição, média, desvio padrão, valores mínimos e máximos, e quartis. A análise de correlação entre as features e com o target ajuda a identificar relações importantes e potenciais problemas de multicolinearidade.\n",
    "\n",
    "```python\n",
    "# Exemplo de código para estatísticas descritivas\n",
    "print(\\'\\nResumo estatístico geral das features:\\')\n",
    "print(train.describe().T)\n",
    "\n",
    "# Exemplo de código para matriz de correlação (após engenharia de features)\n",
    "# Aplicar a engenharia de features para visualização\n",
    "train_eda = engineer_features_improved(train.drop(columns=[\\'id\\', \\'labels\\']))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "corr_matrix = train_eda.corr().abs()\n",
    "sns.heatmap(corr_matrix, cmap=\\'coolwarm\\', annot=False, fmt=\".2f\\')\n",
    "plt.title(\\'Matriz de Correlação entre Features (Após FE)\\')\n",
    "plt.show()\n",
    "\n",
    "# Correlação com o target\n",
    "train_with_labels = engineer_features_improved(train.drop(columns=[\\'id\\']))\n",
    "train_with_labels[\\'labels\\'] = train[\\'labels\\']\n",
    "correlations_with_target = train_with_labels.corrwith(train_with_labels[\\'labels\\']).sort_values(ascending=False)\n",
    "print(\\'\\nCorrelação das Features com o Target:\\')\n",
    "print(correlations_with_target.head(10))\n",
    "print(correlations_with_target.tail(10))\n",
    "```\n",
    "\n",
    "### 5.3. Evolução da Engenharia de Features\n",
    "\n",
    "A função `engineer_features_improved` no código base é o resultado de uma exploração profunda dos dados e da literatura acadêmica. Ela cria uma vasta gama de *features* que buscam capturar aspectos cruciais para o sucesso de startups, como:\n",
    "\n",
    "*   **Proporções e Ratios**: `milestones_per_funding_round`, `avg_funding_per_round`, `relationships_per_milestone`\n",
    "*   **Temporais**: `time_to_first_funding`, `funding_duration`, `time_to_first_milestone`, `milestone_duration`\n",
    "*   **Progressão de Funding**: `funding_progression_score`, `funding_diversity`, `has_later_stage`\n",
    "*   **Geográficas e Setoriais**: `is_innovation_hub`, `is_tech_sector`, `is_high_capital_sector`\n",
    "*   **Team & Network**: `network_density`, `relationship_efficiency`, `team_network_strength`\n",
    "*   **Eficiência Financeira**: `capital_efficiency`, `funding_momentum`, `burn_rate_proxy`\n",
    "*   **Product-Market Fit**: `traction_proxy`, `development_speed`, `market_validation_score`\n",
    "*   **Indicadores de Risco**: `funding_concentration_risk`, `milestone_stagnation_risk`, `low_activity_risk`\n",
    "*   **Maturidade e Escala**: `startup_maturity_score`, `scale_readiness`, `growth_stage_indicator`\n",
    "*   **Timing e Competitividade**: `optimal_funding_timing`, `competitive_positioning`\n",
    "*   **Interações e Composites**: `location_funding_synergy`, `overall_health_score`, `investment_attractiveness`\n",
    "\n",
    "Cada uma dessas *features* é uma hipótese testável sobre os fatores que influenciam o sucesso, e sua criação é um reflexo direto da fase de exploração e visualização dos dados.\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Formulação de Hipóteses\n",
    "\n",
    "Com base na análise exploratória dos dados e no conhecimento do domínio de sucesso de *startups*, formulamos as seguintes hipóteses que podem influenciar a probabilidade de uma empresa ser bem-sucedida:\n",
    "\n",
    "1.  **Hipótese 1: A progressão e diversidade do financiamento impactam positivamente o sucesso da startup.**\n",
    "    *   **Justificativa:** Startups que conseguem múltiplas rodadas de financiamento (`funding_rounds`, `funding_progression_score`) e atraem diferentes tipos de investidores (e.g., `has_VC`, `has_angel`, `funding_diversity`) tendem a ter maior validação de mercado e recursos para escalar suas operações. A capacidade de levantar capital em estágios mais avançados (`has_roundC`, `has_roundD`) é um forte indicativo de confiança dos investidores e potencial de crescimento sustentável. Features como `funding_total_usd`, `avg_funding_per_round` e `funding_velocity` reforçam essa ideia, sugerindo que um volume maior e mais rápido de capital pode impulsionar o sucesso.\n",
    "\n",
    "2.  **Hipótese 2: A capacidade de atingir marcos (milestones) e a eficiência na conversão de recursos em resultados são cruciais para o sucesso.**\n",
    "    *   **Justificativa:** O número de marcos alcançados (`milestones`) e a velocidade com que são atingidos (`milestones_per_funding_round`, `development_speed`) são indicadores de execução e progresso do produto/serviço. Além disso, a eficiência com que a startup utiliza o capital e a rede de contatos para gerar esses marcos (`capital_efficiency`, `relationship_efficiency`) pode diferenciar as empresas bem-sucedidas. Uma alta `traction_proxy` ou `market_validation_score` sugere que a startup está encontrando um bom *product-market fit* e está progredindo de forma eficaz.\n",
    "\n",
    "3.  **Hipótese 3: A localização em hubs de inovação e a atuação em setores de alto crescimento, combinadas com uma forte rede de relacionamentos, aumentam a probabilidade de sucesso.**\n",
    "    *   **Justificativa:** Estar localizado em regiões como Califórnia, Nova Iorque ou Massachusetts (`is_CA`, `is_NY`, `is_MA`, `is_innovation_hub`) oferece acesso a talentos, capital e um ecossistema de *startups* vibrante. Da mesma forma, operar em setores de alto crescimento como software, biotecnologia ou mobile (`is_tech_sector`, `is_high_capital_sector`) pode proporcionar maiores oportunidades de mercado. Uma rede robusta de relacionamentos (`relationships`, `network_density`, `team_network_strength`) facilita parcerias, mentoria e acesso a recursos essenciais, potencializando o crescimento e a resiliência da startup.\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Seleção de Features\n",
    "\n",
    "A seleção de *features* é um processo crítico para reduzir a dimensionalidade dos dados, remover ruídos e melhorar o desempenho e a interpretabilidade do modelo. O pipeline final utiliza uma abordagem baseada em modelo para a seleção de features.\n",
    "\n",
    "### 7.1. Engenharia de Features Avançada\n",
    "\n",
    "A primeira e mais importante etapa da seleção de features é a engenharia de features, implementada na função `engineer_features_improved`. Esta função cria uma vasta gama de features a partir das variáveis originais, incorporando conhecimento de domínio e hipóteses sobre o sucesso de startups. Este processo é, em si, uma forma de seleção, pois foca na criação de variáveis mais informativas e preditivas.\n",
    "\n",
    "```python\n",
    "# Função engineer_features_improved (trecho adaptado para o notebook)\n",
    "def engineer_features_improved(df):\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. FEATURES ORIGINAIS MANTIDAS (bem alinhadas com literatura)\n",
    "    df_enhanced[\\'milestones_per_funding_round\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'funding_rounds\\'] + 0.1)\n",
    "    df_enhanced[\\'avg_funding_per_round\\'] = df_enhanced[\\'funding_total_usd\\'] / (df_enhanced[\\'funding_rounds\\'] + 0.1)\n",
    "    df_enhanced[\\'relationships_per_milestone\\'] = df_enhanced[\\'relationships\\'] / (df_enhanced[\\'milestones\\'] + 0.1)\n",
    "    \n",
    "    df_enhanced[\\'time_to_first_funding\\'] = df_enhanced[\\'age_first_funding_year\\'].fillna(999)\n",
    "    df_enhanced[\\'funding_duration\\'] = (df_enhanced[\\'age_last_funding_year\\'] - df_enhanced[\\'age_first_funding_year\\']).fillna(0)\n",
    "    df_enhanced[\\'time_to_first_milestone\\'] = df_enhanced[\\'age_first_milestone_year\\'].fillna(999)\n",
    "    df_enhanced[\\'milestone_duration\\'] = (df_enhanced[\\'age_last_milestone_year\\'] - df_enhanced[\\'age_first_milestone_year\\']).fillna(0)\n",
    "    \n",
    "    df_enhanced[\\'funding_progression_score\\'] = (\n",
    "        df_enhanced[\\'has_roundA\\'] * 1 +\n",
    "        df_enhanced[\\'has_roundB\\'] * 2 + \n",
    "        df_enhanced[\\'has_roundC\\'] * 3 +\n",
    "        df_enhanced[\\'has_roundD\\'] * 4\n",
    "    )\n",
    "    df_enhanced[\\'funding_diversity\\'] = df_enhanced[\\'has_VC\\'] + df_enhanced[\\'has_angel\\']\n",
    "    df_enhanced[\\'has_later_stage\\'] = ((df_enhanced[\\'has_roundC\\'] == 1) | (df_enhanced[\\'has_roundD\\'] == 1)).astype(int)\n",
    "    \n",
    "    # 2. MELHORIAS NAS FEATURES GEOGRÁFICAS E SETORIAIS\n",
    "    df_enhanced[\\'is_innovation_hub\\'] = (\n",
    "        (df_enhanced[\\'is_CA\\'] == 1) | \n",
    "        (df_enhanced[\\'is_NY\\'] == 1) | \n",
    "        (df_enhanced[\\'is_MA\\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'is_secondary_hub\\'] = (df_enhanced[\\'is_TX\\'] == 1).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'is_tech_sector\\'] = (\n",
    "        (df_enhanced[\\'is_software\\'] == 1) |\n",
    "        (df_enhanced[\\'is_web\\'] == 1) |\n",
    "        (df_enhanced[\\'is_mobile\\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'is_high_capital_sector\\'] = (\n",
    "        (df_enhanced[\\'is_biotech\\'] == 1) |\n",
    "        (df_enhanced[\\'is_enterprise\\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'is_consumer_facing\\'] = (\n",
    "        (df_enhanced[\\'is_ecommerce\\'] == 1) |\n",
    "        (df_enhanced[\\'is_advertising\\'] == 1) |\n",
    "        (df_enhanced[\\'is_gamesvideo\\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 3. NOVAS FEATURES BASEADAS NA LITERATURA\n",
    "    df_enhanced[\\'network_density\\'] = df_enhanced[\\'relationships\\'] / (df_enhanced[\\'funding_rounds\\'] + 1)\n",
    "    df_enhanced[\\'relationship_efficiency\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'relationships\\'] + 0.1)\n",
    "    df_enhanced[\\'team_network_strength\\'] = df_enhanced[\\'relationships\\'] * df_enhanced[\\'milestones\\'] / 10\n",
    "    df_enhanced[\\'founder_experience_proxy\\'] = np.minimum(df_enhanced[\\'relationships\\'] / 5, 3)\n",
    "    df_enhanced[\\'team_size_proxy\\'] = np.log1p(df_enhanced[\\'relationships\\']) / 2\n",
    "    \n",
    "    df_enhanced[\\'capital_efficiency\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'funding_total_usd\\'] / 1000000 + 0.1)\n",
    "    df_enhanced[\\'funding_momentum\\'] = df_enhanced[\\'funding_rounds\\'] / (df_enhanced[\\'age_last_funding_year\\'] + 0.1)\n",
    "    df_enhanced[\\'funding_velocity\\'] = df_enhanced[\\'funding_total_usd\\'] / (df_enhanced[\\'age_last_funding_year\\'] + 0.1)\n",
    "    df_enhanced[\\'burn_rate_proxy\\'] = df_enhanced[\\'funding_total_usd\\'] / (df_enhanced[\\'funding_duration\\'] + 0.1)\n",
    "    df_enhanced[\\'runway_efficiency\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'burn_rate_proxy\\'] + 0.1)\n",
    "    \n",
    "    df_enhanced[\\'traction_proxy\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'time_to_first_funding\\'] + 1)\n",
    "    df_enhanced[\\'development_speed\\'] = df_enhanced[\\'milestones\\'] / (df_enhanced[\\'funding_rounds\\'] + 0.1)\n",
    "    df_enhanced[\\'market_validation_score\\'] = (\n",
    "        (df_enhanced[\\'milestones\\'] > 0).astype(int) +\n",
    "        (df_enhanced[\\'funding_rounds\\'] > 1).astype(int) +\n",
    "        (df_enhanced[\\'has_VC\\'] == 1).astype(int)\n",
    "    )\n",
    "    df_enhanced[\\'customer_traction_proxy\\'] = df_enhanced[\\'relationships\\'] * df_enhanced[\\'milestones\\'] / (df_enhanced[\\'funding_rounds\\'] + 1)\n",
    "    df_enhanced[\\'product_maturity\\'] = np.minimum(df_enhanced[\\'milestone_duration\\'] / 2, 5)\n",
    "    \n",
    "    df_enhanced[\\'funding_concentration_risk\\'] = df_enhanced[\\'avg_funding_per_round\\'] / (df_enhanced[\\'funding_total_usd\\'] + 0.1)\n",
    "    df_enhanced[\\'milestone_stagnation_risk\\'] = (df_enhanced[\\'age_last_milestone_year\\'].fillna(999) > 3).astype(int)\n",
    "    df_enhanced[\\'late_funding_risk\\'] = (df_enhanced[\\'time_to_first_funding\\'] > 2).astype(int)\n",
    "    df_enhanced[\\'funding_gap_risk\\'] = np.maximum(0, df_enhanced[\\'age_last_funding_year\\'] - 2)\n",
    "    df_enhanced[\\'low_activity_risk\\'] = (\n",
    "        (df_enhanced[\\'milestones\\'] == 0) | \n",
    "        (df_enhanced[\\'relationships\\'] < 3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'startup_maturity_score\\'] = (\n",
    "        (df_enhanced[\\'milestones\\'] >= 1).astype(int) +\n",
    "        (df_enhanced[\\'funding_rounds\\'] >= 2).astype(int) +\n",
    "        (df_enhanced[\\'relationships\\'] >= 5).astype(int) +\n",
    "        (df_enhanced[\\'has_later_stage\\'] == 1).astype(int) * 2\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'scale_readiness\\'] = (\n",
    "        (df_enhanced[\\'funding_progression_score\\'] >= 2) &\n",
    "        (df_enhanced[\\'milestones\\'] >= 2) &\n",
    "        (df_enhanced[\\'relationships\\'] >= 3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'growth_stage_indicator\\'] = np.minimum(\n",
    "        df_enhanced[\\'funding_progression_score\\'] + df_enhanced[\\'startup_maturity_score\\'], 8\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'optimal_funding_timing\\'] = (\n",
    "        (df_enhanced[\\'time_to_first_funding\\'] >= 0.5) & \n",
    "        (df_enhanced[\\'time_to_first_funding\\'] <= 2.0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\\'funding_consistency\\'] = 1 / (df_enhanced[\\'funding_duration\\'] + 0.1)\n",
    "    df_enhanced[\\'competitive_positioning\\'] = (\n",
    "        df_enhanced[\\'is_innovation_hub\\'].astype(int) * 2 +\n",
    "        df_enhanced[\\'is_tech_sector\\'].astype(int) +\n",
    "        (df_enhanced[\\'funding_total_usd\\'] > df_enhanced[\\'funding_total_usd\\'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'location_funding_synergy\\'] = (\n",
    "        df_enhanced[\\'is_innovation_hub\\'] * np.log1p(df_enhanced[\\'funding_total_usd\\'])\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'sector_funding_fit\\'] = (\n",
    "        df_enhanced[\\'is_high_capital_sector\\'] * df_enhanced[\\'funding_progression_score\\']\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'team_market_fit\\'] = df_enhanced[\\'network_density\\'] * df_enhanced[\\'market_validation_score\\']\n",
    "    df_enhanced[\\'execution_capability\\'] = df_enhanced[\\'development_speed\\'] * df_enhanced[\\'capital_efficiency\\']\n",
    "    \n",
    "    df_enhanced[\\'overall_health_score\\'] = (\n",
    "        df_enhanced[\\'startup_maturity_score\\'] * 0.3 +\n",
    "        df_enhanced[\\'market_validation_score\\'] * 0.4 +\n",
    "        df_enhanced[\\'competitive_positioning\\'] * 0.3\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\\'investment_attractiveness\\'] = (\n",
    "        df_enhanced[\\'funding_progression_score\\'] * 0.4 +\n",
    "        df_enhanced[\\'capital_efficiency\\'] * 0.3 +\n",
    "        df_enhanced[\\'network_density\\'] * 0.3\n",
    "    )\n",
    "\n",
    "    df_enhanced[\\'funding_recency\\'] = 10 - df_enhanced[\\'age_last_funding_year\\'].fillna(10)\n",
    "    df_enhanced[\\'milestone_recency\\'] = 10 - df_enhanced[\\'age_last_milestone_year\\'].fillna(10)\n",
    "    df_enhanced[\\'activity_recency_score\\'] = df_enhanced[\\'funding_recency\\'] + df_enhanced[\\'milestone_recency\\']\n",
    "\n",
    "    df_enhanced[\\'high_quality_network\\'] = (df_enhanced[\\'relationships\\'] > df_enhanced[\\'relationships\\'].quantile(0.75)).astype(int)\n",
    "    df_enhanced[\\'rapid_development\\'] = (df_enhanced[\\'development_speed\\'] > df_enhanced[\\'development_speed\\'].quantile(0.75)).astype(int) # Adicionado\n",
    "    df_enhanced[\\'is_well_funded\\'] = (df_enhanced[\\'funding_total_usd\\'] / 1000000 > (df_enhanced[\\'funding_total_usd\\'] / 1000000).quantile(0.75)).astype(int) # Adicionado\n",
    "    df_enhanced[\\'success_signals\\'] = df_enhanced[\\'has_later_stage\\'] + df_enhanced[\\'is_innovation_hub\\'] + df_enhanced[\\'high_quality_network\\'] + df_enhanced[\\'rapid_development\\'] + df_enhanced[\\'is_well_funded\\']\n",
    "    \n",
    "    return df_enhanced\n",
    "```"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Construção e Avaliação do Modelo\n",
    "\n",
    "A construção e avaliação do modelo são etapas centrais em qualquer projeto de *machine learning*. O pipeline final (`create_advanced_ml_pipeline`) é projetado para maximizar a performance, utilizando um *ensemble* de modelos e técnicas avançadas de balanceamento de classes e seleção de features.\n",
    "\n",
    "### 8.1. Pipeline de Machine Learning Otimizado\n",
    "\n",
    "O pipeline de ML é construído com as seguintes etapas:\n",
    "\n",
    "1.  **Scaler Robusto (`RobustScaler`)**: Escala as features usando a mediana e o intervalo interquartil, tornando-o robusto a *outliers*.\n",
    "2.  **Imputação de Valores Ausentes (`SimpleImputer`)**: Preenche quaisquer valores ausentes remanescentes com a mediana. Esta etapa é um *fallback* após o tratamento mais inteligente na função `advanced_preprocessing_pipeline`.\n",
    "3.  **Seleção de Features Automática (`RFECV`)**: Utiliza a Eliminação Recursiva de Features com Validação Cruzada, guiada por um `RandomForestClassifier`, para selecionar o subconjunto ótimo de features que maximiza o `roc_auc`. Isso garante que apenas as features mais relevantes sejam usadas, reduzindo ruído e potencial *overfitting*. `min_features_to_select=15` define um limite mínimo de features.\n",
    "4.  **Balanceamento de Classes (`SMOTE`)**: Aplica a técnica SMOTE (Synthetic Minority Over-sampling Technique) para gerar amostras sintéticas da classe minoritária, balanceando o dataset de treino e melhorando a capacidade do modelo de aprender sobre a classe de sucesso. `k_neighbors=3` é um parâmetro ajustado para a geração de vizinhos.\n",
    "5.  **Ensemble de Modelos (`VotingClassifier`)**: Combina as previsões de três modelos robustos: `RandomForestClassifier`, `XGBClassifier` e `LGBMClassifier`. O `voting=\'soft\'` utiliza as probabilidades de cada modelo, ponderando-as para uma previsão final mais robusta e com menor variância.\n",
    "\n",
    "```python\n",
    "# Função create_advanced_ml_pipeline\n",
    "def create_advanced_ml_pipeline():\n",
    "    feature_selector = RFECV(\n",
    "        estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        step=1,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring=\'roc_auc\',\n",
    "        min_features_to_select=15,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    models = {\n",
    "        \'rf\': RandomForestClassifier(\n",
    "            n_estimators=150, max_depth=12, min_samples_split=5, min_samples_leaf=2,\n",
    "            class_weight=\'balanced\', random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \'xgb\': XGBClassifier(\n",
    "            n_estimators=150, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \'lgb\': LGBMClassifier(\n",
    "            n_estimators=150, max_depth=6, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8,\n",
    "            class_weight=\'balanced\', random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=list(models.items()),\n",
    "        voting=\'soft\',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "      (\'scaler\', scaler),\n",
    "      (\'imputer\', SimpleImputer(strategy=\'median\')), # Fallback imputer\n",
    "      (\'feature_selection\', feature_selector),\n",
    "      (\'smote\', smote),\n",
    "      (\'ensemble\', ensemble)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "```\n",
    "\n",
    "### 8.2. Avaliação Abrangente do Modelo\n",
    "\n",
    "A função `comprehensive_model_evaluation` realiza uma avaliação detalhada do modelo, utilizando um conjunto de métricas robustas para problemas de classificação desbalanceada:\n",
    "\n",
    "*   **AUC-ROC**: Principal métrica para avaliar a capacidade de discriminação do modelo.\n",
    "*   **AUC-PR (Area Under the Precision-Recall Curve)**: Mais informativa que a AUC-ROC para datasets altamente desbalanceados, pois foca na performance da classe positiva.\n",
    "*   **F1-Score, Precision, Recall**: Métricas essenciais para entender o balanço entre falsos positivos e falsos negativos.\n",
    "*   **Classification Report e Matriz de Confusão**: Fornecem uma visão detalhada do desempenho por classe.\n",
    "*   **Otimização de Threshold**: Identifica o *threshold* de probabilidade que maximiza o F1-score, permitindo ajustar o ponto de corte para as previsões binárias de acordo com o objetivo do negócio.\n",
    "\n",
    "```python\n",
    "# Função comprehensive_model_evaluation (trecho adaptado para o notebook)\n",
    "def comprehensive_model_evaluation(pipeline, X_train, y_train, X_val, y_val):\n",
    "    print(\"\\n=== AVALIAÇÃO ABRANGENTE DO MODELO ===\")\n",
    "    print(\"Treinando pipeline completo...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    y_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_val, y_prob)\n",
    "    auc_pr = average_precision_score(y_val, y_prob)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS PRINCIPAIS:\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"AUC-PR: {auc_pr:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\nRELATÓRIO DETALHADO:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "    print(f\"\\nMATRIZ DE CONFUSÃO:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_val, y_prob)\n",
    "    f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(f\"\\nOTIMIZAÇÃO DE THRESHOLD:\")\n",
    "    print(f\"Threshold ótimo: {optimal_threshold:.3f}\")\n",
    "    print(f\"F1-Score ótimo: {f1_scores[optimal_idx]:.4f}\")\n",
    "    \n",
    "    return pipeline, optimal_threshold, {\n",
    "        \'auc_roc\': auc_roc,\n",
    "        \'auc_pr\': auc_pr,\n",
    "        \'f1\': f1,\n",
    "        \'precision\': precision,\n",
    "        \'recall\': recall,\n",
    "        \'optimal_threshold\': optimal_threshold\n",
    "    }\n",
    "```\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Finetuning de Hiperparâmetros\n",
    "\n",
    "O ajuste fino (*finetuning*) dos hiperparâmetros é uma etapa crucial para otimizar o desempenho dos modelos de *machine learning*. No pipeline final, os hiperparâmetros dos modelos que compõem o `VotingClassifier` (RandomForest, XGBoost, LightGBM) são definidos com valores que foram provavelmente otimizados em etapas anteriores de experimentação (como `RandomizedSearchCV` ou `GridSearchCV`).\n",
    "\n",
    "### 9.1. Hiperparâmetros dos Modelos no Ensemble\n",
    "\n",
    "Os modelos no `VotingClassifier` são configurados com hiperparâmetros específicos:\n",
    "\n",
    "*   **RandomForestClassifier**:\n",
    "    *   `n_estimators=150`: Número de árvores.\n",
    "    *   `max_depth=12`: Profundidade máxima das árvores.\n",
    "    *   `min_samples_split=5`, `min_samples_leaf=2`: Controles de divisão e folhas para evitar *overfitting*.\n",
    "    *   `class_weight=\'balanced\'` : Lida com o desbalanceamento de classes.\n",
    "\n",
    "*   **XGBClassifier**:\n",
    "    *   `n_estimators=150`, `max_depth=6`, `learning_rate=0.1`: Parâmetros de controle do *boosting*.\n",
    "    *   `subsample=0.8`, `colsample_bytree=0.8`: Reduzem o *overfitting* através de subamostragem de linhas e colunas.\n",
    "\n",
    "*   **LGBMClassifier**:\n",
    "    *   `n_estimators=150`, `max_depth=6`, `learning_rate=0.1`: Similar ao XGBoost.\n",
    "    *   `feature_fraction=0.8`, `bagging_fraction=0.8`: Controles de subamostragem.\n",
    "    *   `class_weight=\'balanced\'` : Lida com o desbalanceamento de classes.\n",
    "\n",
    "Esses valores são o resultado de um processo de otimização que pode ter envolvido `RandomizedSearchCV` ou `GridSearchCV` em etapas de desenvolvimento anteriores, buscando o melhor equilíbrio entre performance e generalização.\n",
    "\n",
    "### 9.2. Otimização de Threshold\n",
    "\n",
    "A função `comprehensive_model_evaluation` inclui uma etapa crucial de otimização do *threshold* de classificação. Após obter as probabilidades de previsão do modelo, um *threshold* ideal é selecionado para maximizar o F1-score. Esta etapa é fundamental, pois o *threshold* padrão de 0.5 pode não ser o ideal para problemas de classificação com classes desbalanceadas ou quando se deseja priorizar uma métrica específica.\n",
    "\n",
    "```python\n",
    "# Trecho da função comprehensive_model_evaluation para otimização de threshold\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_val, y_prob)\n",
    "    f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(f\"\\nOTIMIZAÇÃO DE THRESHOLD:\")\n",
    "    print(f\"Threshold ótimo: {optimal_threshold:.3f}\")\n",
    "    print(f\"F1-Score ótimo: {f1_scores[optimal_idx]:.4f}\")\n",
    "```\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Acurácia Mínima e Análise de Métricas\n",
    "\n",
    "O critério de avaliação estabelece uma **acurácia mínima de 80%** para que o modelo seja considerado bem-sucedido. O pipeline final é projetado para atingir e superar essa meta, utilizando uma combinação de engenharia de features avançada, seleção de features baseada em modelo, balanceamento de classes e um *ensemble* de modelos robustos.\n",
    "\n",
    "### 10.1. Avaliação da Acurácia e Outras Métricas\n",
    "\n",
    "A função `comprehensive_model_evaluation` fornece todas as métricas necessárias para uma análise completa:\n",
    "\n",
    "*   **AUC-ROC e AUC-PR**: Indicadores robustos da capacidade de discriminação do modelo, especialmente em datasets desbalanceados.\n",
    "*   **F1-Score, Precision, Recall**: Essenciais para entender o desempenho do modelo em relação a falsos positivos e falsos negativos, crucial para o problema de predição de sucesso de startups.\n",
    "*   **Classification Report e Matriz de Confusão**: Detalham o desempenho por classe, permitindo identificar onde o modelo está acertando e errando.\n",
    "\n",
    "A combinação dessas métricas, juntamente com a otimização do *threshold*, permite uma avaliação precisa e alinhada aos objetivos do projeto.\n",
    "\n",
    "### 10.2. Análise de Importância das Features\n",
    "\n",
    "A função `feature_importance_analysis` utiliza as importâncias das features dos modelos no *ensemble* para identificar quais variáveis são mais relevantes para as previsões. Isso não só ajuda na interpretabilidade do modelo, mas também valida as hipóteses formuladas e as decisões de engenharia de features.\n",
    "\n",
    "```python\n",
    "# Função feature_importance_analysis (trecho adaptado para o notebook)\n",
    "def feature_importance_analysis(pipeline, feature_names):\n",
    "    print(\"\\n=== ANÁLISE DE IMPORTÂNCIA DAS FEATURES ===\")\n",
    "    try:\n",
    "        ensemble = pipeline.named_steps[\\\'ensemble\\\"]\n",
    "        importances = np.zeros(len(feature_names))\n",
    "        \n",
    "        for name, model in ensemble.estimators_:\n",
    "            if hasattr(model, \\\'feature_importances_\\\"]:\n",
    "                importances += model.feature_importances_\n",
    "        \n",
    "        importances /= len(ensemble.estimators_)\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            \\\'feature\\\": feature_names,\n",
    "            \\\'importance\\\": importances\n",
    "        }).sort_values(\\'importance\\\', ascending=False)\n",
    "        \n",
    "        print(\"TOP 10 Features Mais Importantes:\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.barplot(x=\\'importance\\\', y=\\'feature\\\', data=feature_importance_df.head(10))\n",
    "        plt.title(\\'Top 10 Feature Importances (Ensemble)\\' )\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\\"Erro ao calcular feature importances: {e}\\")\n",
    "\n",
    "# Exemplo de uso (assumindo X_train_processed e y_train_processed são os dados após o pré-processamento e balanceamento)\n",
    "# feature_names = X_train_processed.columns # Obter nomes das features após RFECV\n",
    "# feature_importance_analysis(final_pipeline, feature_names)\n",
    "```\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Documentação e Apresentação dos Resultados\n",
    "\n",
    "Este notebook foi estruturado para apresentar de forma clara e concisa todo o processo de desenvolvimento do modelo preditivo de sucesso de *startups*, desde a análise inicial dos dados até a otimização e avaliação final. A organização segue uma progressão lógica, espelhando as etapas de um projeto de *machine learning*.\n",
    "\n",
    "### 11.1. Organização do Notebook\n",
    "\n",
    "*   **Estrutura Sequencial**: O notebook é dividido em seções numeradas, cada uma abordando um aspecto específico do projeto (Introdução, Descrição do Dataset, Limpeza de Dados, etc.). Isso facilita a navegação e a compreensão do fluxo de trabalho.\n",
    "*   **Código Limpo e Comentado**: Todos os trechos de código apresentados são exemplos extraídos e adaptados para o contexto do notebook. O código é mantido o mais limpo possível, com comentários explicativos onde necessário para detalhar a funcionalidade e as decisões tomadas.\n",
    "*   **Células de Texto Explicativas**: Cada seção é precedida por células de texto (`Markdown`) que fornecem o contexto, explicam as metodologias aplicadas, justificam as escolhas e interpretam os resultados. A evolução das abordagens ao longo dos diferentes arquivos Python é destacada para demonstrar o processo iterativo de refinamento.\n",
    "*   **Visualizações Integradas**: As visualizações geradas durante a Análise Exploratória de Dados (EDA) e a avaliação do modelo (e.g., matrizes de confusão, curvas ROC) são exibidas diretamente para ilustrar os *insights* e o desempenho do modelo.\n",
    "\n",
    "### 11.2. Justificativas para as Decisões\n",
    "\n",
    "Ao longo do notebook, as justificativas para as principais decisões são apresentadas de forma objetiva:\n",
    "\n",
    "*   **Tratamento de Nulos e Outliers**: A escolha por imputação por mediana, e *clipping* de *outliers* é justificada pela robustez dessas técnicas e pela necessidade de preservar informações críticas em certas *features*. O uso de `RobustScaler` complementa essa estratégia.\n",
    "*   **Codificação de Categóricas**: A decisão de confiar nas *dummies* existentes e nas features engenheiradas para informações setoriais e geográficas é explicada pela busca por redução de dimensionalidade e pela eficácia dessas representações.\n",
    "*   **Engenharia e Seleção de Features**: A criação de novas *features* é fundamentada em hipóteses de domínio e na busca por variáveis que melhor representem os fatores de sucesso de *startups*. A seleção de *features* baseada em modelos (`RFECV`) visa otimizar a performance e a interpretabilidade.\n",
    "*   **Modelagem e Finetuning**: A escolha de um *ensemble* de modelos (RandomForest, XGBoost, LightGBM) é justificada por sua eficácia em problemas de classificação e pela capacidade de reduzir a variância. A otimização do *threshold* é destacada como crucial para lidar com o desbalanceamento de classes e maximizar métricas relevantes como o F1-score.\n",
    "*   **Métricas de Avaliação**: A ênfase em métricas como ROC-AUC, F1-score, precisão e *recall*, além da acurácia, é justificada pela natureza desbalanceada do dataset e pela necessidade de uma avaliação abrangente do desempenho do modelo.\n",
    "\n",
    "O objetivo final é fornecer um documento completo e compreensível que não apenas apresente os resultados, mas também detalhe o raciocínio por trás de cada etapa, permitindo que o leitor entenda a lógica e a evolução do projeto.\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Submissão\n",
    "\n",
    "O entregável final do projeto é um arquivo CSV contendo as previsões do modelo para o conjunto de dados de teste (`test.csv`), no formato especificado (`id`, `labels`). A geração deste arquivo é realizada após a seleção do melhor modelo e a otimização do *threshold* de classificação.\n",
    "\n",
    "### 12.1. Processo de Geração do Arquivo de Submissão\n",
    "\n",
    "1.  **Seleção do Melhor Modelo**: Após as etapas de treinamento, validação cruzada e *finetuning* de hiperparâmetros, o modelo com o melhor desempenho (o `ensemble` treinado) é utilizado.\n",
    "2.  **Previsão no Conjunto de Teste**: O modelo selecionado é então utilizado para prever as probabilidades de sucesso (`predict_proba`) para o conjunto de dados de teste (`X_test_processed`).\n",
    "3.  **Aplicação do Threshold Otimizado**: As probabilidades são convertidas em classes binárias (0 ou 1) aplicando-se o *threshold* de classificação que foi otimizado para maximizar o F1-score no conjunto de validação.\n",
    "4.  **Criação do DataFrame de Submissão**: Um DataFrame é criado com as colunas `id` (do dataset de teste original) e `labels` (as previsões binárias).\n",
    "5.  **Exportação para CSV**: O DataFrame é exportado para um arquivo CSV, sem o índice, conforme o formato exigido.\n",
    "\n",
    "```python\n",
    "# Exemplo de código para gerar o arquivo de submissão\n",
    "# Assumindo que final_pipeline é o pipeline treinado e optimal_threshold é o threshold encontrado\n",
    "# X_test_final = engineer_features_improved(test.drop(columns=[\\\'id\\\"]))\n",
    "# X_test_final, _ = advanced_preprocessing_pipeline(X_test_final.copy(), None, X_test_final.copy()) # Apenas para aplicar o pré-processamento\n",
    "#\n",
    "# # O RFECV dentro do pipeline já fará a seleção de features no X_test_final\n",
    "# final_proba_test = final_pipeline.predict_proba(X_test_final)[:, 1]\n",
    "# final_preds = (final_proba_test > optimal_threshold).astype(int)\n",
    "#\n",
    "# submission = pd.DataFrame({\\'id\\\': test[\\\'id\\\"], \\\'labels\\\': final_preds})\n",
    "# submission.to_csv(\\'submission.csv\\\', index=False)\n",
    "# print(\"Arquivo de submissão gerado: submission.csv\")\n",
    "```\n"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Seção Extra: Evolução do Código\n",
    "\n",
    "Esta seção apresenta as abordagens e códigos explorados em etapas anteriores do desenvolvimento do projeto, antes da versão final otimizada. Ela serve como um registro da evolução das ideias e técnicas aplicadas.\n",
    "\n",
    "### 13.1. Abordagens Iniciais (Baseadas em Iterações Anteriores)\n",
    "\n",
    "As primeiras iterações do projeto focaram em uma engenharia de features mais direta e na aplicação de modelos como Random Forest e XGBoost com otimização de hiperparâmetros via `GridSearchCV` e `RandomizedSearchCV`.\n",
    "\n",
    "#### 13.1.1. Engenharia de Features e Modelagem Inicial (Baseado em `01.py` e `02.py`)\n",
    "\n",
    "Nestas fases iniciais, foram criadas features como `funding_per_round`, `milestones_per_year`, e interações regionais/setoriais. O pré-processamento utilizava `SimpleImputer` para mediana e `StandardScaler` para features contínuas, e `OneHotEncoder` para `category_code`. Modelos como `RandomForestClassifier` e `XGBClassifier` foram treinados e avaliados com validação cruzada e busca de hiperparâmetros.\n",
    "\n",
    "```python\n",
    "# Código adaptado das iterações iniciais (01.py e 02.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "X_initial = train.drop([\'id\', \'labels\'], axis=1)\n",
    "y_initial = train[\'labels\']\n",
    "X_test_initial = test.drop([\'id\'], axis=1)\n",
    "\n",
    "for df in [X_initial, X_test_initial]:\n",
    "    df[\'funding_per_round\'] = df[\'funding_total_usd\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'milestones_per_year\'] = df[\'milestones\'] / (df[\'age_last_milestone_year\'].fillna(df[\'age_last_milestone_year\'].median()) + 1)\n",
    "    df[\'funding_per_year\'] = df[\'funding_total_usd\'] / (df[\'age_last_funding_year\'].fillna(df[\'age_last_funding_year\'].median()) + 1)\n",
    "    df[\'participants_per_round\'] = df[\'avg_participants\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'relationships_per_round\'] = df[\'relationships\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'is_CA_software\'] = df[\'is_CA\'] * df[\'is_software\']\n",
    "    df[\'is_NY_advertising\'] = df[\'is_NY\'] * df[\'is_advertising\']\n",
    "    df[\'is_MA_biotech\'] = df[\'is_MA\'] * df[\'is_biotech\']\n",
    "    df[\'is_high_funding\'] = (df[\'funding_total_usd\'] > df[\'funding_total_usd\'].median()).astype(int)\n",
    "    df[\'is_high_milestones\'] = (df[\'milestones\'] > df[\'milestones\'].median()).astype(int)\n",
    "    df[\'time_to_first_milestone_after_funding\'] = (\n",
    "        df[\'age_first_milestone_year\'] - df[\'age_first_funding_year\']\n",
    "    )\n",
    "    df[\'milestone_per_million\'] = df[\'milestones\'] / (df[\'funding_total_usd\'] / 1e6 + 1)\n",
    "\n",
    "    # Features avançadas de pesquisa em sucesso de startups (do 02.py)\n",
    "    df[\'has_multiple_funding_types\'] = ((df[\'has_VC\'] + df[\'has_angel\']) >= 2).astype(int)\n",
    "    df[\'advanced_funding_rounds\'] = ((df[\'has_roundC\'] + df[\'has_roundD\']) >= 1).astype(int)\n",
    "    df[\'funding_progression_score\'] = (df[\'has_roundA\'] + df[\'has_roundB\'] + df[\'has_roundC\'] + df[\'has_roundD\'])\n",
    "    df[\'funding_velocity\'] = df[\'funding_total_usd\'] / (df[\'age_last_funding_year\'].fillna(1) + 1)\n",
    "    df[\'milestone_momentum\'] = df[\'milestones\'] / (df[\'age_last_milestone_year\'].fillna(1) + 1)\n",
    "    df[\'network_density\'] = df[\'relationships\'] * df[\'avg_participants\'] / (df[\'funding_rounds\'] + 1)\n",
    "    df[\'ecosystem_strength\'] = df[\'relationships\'] + df[\'milestones\'] + df[\'funding_rounds\']\n",
    "    df[\'traction_score\'] = (df[\'funding_total_usd\'] / 1000000) * df[\'milestones\'] * df[\'relationships\']\n",
    "    df[\'is_high_traction\'] = (df[\'traction_score\'] > df[\'traction_score\'].quantile(0.75)).astype(int)\n",
    "    df[\'efficiency_ratio\'] = df[\'milestones\'] / (df[\'funding_total_usd\'] / 1000000 + 1)\n",
    "    df[\'capital_efficiency\'] = df[\'relationships\'] / (df[\'funding_total_usd\'] / 1000000 + 1)\n",
    "    df[\'funding_milestone_alignment\'] = np.abs(\n",
    "        df[\'age_last_funding_year\'].fillna(0) - df[\'age_last_milestone_year\'].fillna(0)\n",
    "    )\n",
    "    df[\'is_well_timed_funding\'] = (df[\'funding_milestone_alignment\'] <= 1).astype(int)\n",
    "    df[\'is_high_growth_sector\'] = (\n",
    "        df[\'is_software\'] + df[\'is_biotech\'] + df[\'is_mobile\'] + df[\'is_enterprise\']\n",
    "    ).astype(int)\n",
    "    df[\'is_traditional_sector\'] = (\n",
    "        df[\'is_consulting\'] + df[\'is_advertising\']\n",
    "    ).astype(int)\n",
    "    df[\'is_innovation_hub\'] = (df[\'is_CA\'] + df[\'is_NY\'] + df[\'is_MA\']).astype(int)\n",
    "    df[\'innovation_hub_tech\'] = df[\'is_innovation_hub\'] * df[\'is_high_growth_sector\']\n",
    "    df[\'scale_readiness\'] = (\n",
    "        (df[\'funding_rounds\'] >= 3).astype(int) + \n",
    "        (df[\'relationships\'] >= df[\'relationships\'].median()).astype(int) +\n",
    "        (df[\'milestones\'] >= df[\'milestones\'].median()).astype(int)\n",
    "    )\n",
    "    df[\'diversified_funding\'] = df[\'funding_progression_score\'] * df[\'has_multiple_funding_types\']\n",
    "    df[\'sustainable_growth_pattern\'] = (\n",
    "        df[\'is_well_timed_funding\'] * df[\'efficiency_ratio\'] * df[\'network_density\']\n",
    "    )\n",
    "    df[\'competitive_moat\'] = df[\'milestones\'] * df[\'relationships\'] / (df[\'funding_rounds\'] + 1)\n",
    "    df[\'market_position_strength\'] = (\n",
    "        df[\'funding_total_usd\'] / (df[\'funding_total_usd\'].median() + 1)\n",
    "    ) * df[\'milestone_momentum\']\n",
    "\n",
    "cont_features_initial = [\n",
    "    \'age_first_funding_year\', \'age_last_funding_year\',\n",
    "    \'age_first_milestone_year\', \'age_last_milestone_year\',\n",
    "    \'relationships\', \'funding_rounds\', \'funding_total_usd\',\n",
    "    \'milestones\', \'avg_participants\', \'relationships_per_round\',\n",
    "    \'participants_per_round\', \'funding_per_year\', \'milestones_per_year\',\n",
    "    \'funding_per_round\', \'time_to_first_milestone_after_funding\',\n",
    "    \'funding_velocity\', \'milestone_momentum\', \'network_density\', \'ecosystem_strength\',\n",
    "    \'traction_score\', \'efficiency_ratio\', \'capital_efficiency\', \'funding_milestone_alignment\',\n",
    "    \'competitive_moat\', \'market_position_strength\', \n",
    "    \'funding_progression_score\', \'scale_readiness\', \'diversified_funding\', \'sustainable_growth_pattern\'
",
    "]\n",
    "cat_features_initial = [\'category_code\']\n",
    "bin_features_initial = [col for col in X_initial.columns if col.startswith(\'is_\') or col.startswith(\'has_\') and col != \'is_othercategory\']\n",
    "bin_features_initial += [\n",
    "    \'has_multiple_funding_types\', \'advanced_funding_rounds\', \'is_high_traction\',\n",
    "    \'is_well_timed_funding\', \'is_high_growth_sector\', \'is_traditional_sector\',\n",
    "    \'is_innovation_hub\', \'innovation_hub_tech\'
",
    "]\n",
    "\n",
    "cont_transformer_initial = Pipeline([\n",
    "    (\'imputer\', SimpleImputer(strategy=\'median\')),\n",
    "    (\'scaler\', StandardScaler())\n",
    "]) \n",
    "cat_transformer_initial = Pipeline([\n",
    "    (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))\n",
    "]) \n",
    "preprocessor_initial = ColumnTransformer([\n",
    "    (\'cont\', cont_transformer_initial, cont_features_initial),\n",
    "    (\'cat\', cat_transformer_initial, cat_features_initial),\n",
    "    (\'bin\', \'passthrough\', bin_features_initial)\n",
    "]) \n",
    "\n",
    "X_train_split_initial, X_val_initial, y_train_split_initial, y_val_initial = train_test_split(X_initial, y_initial, stratify=y_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "rf_pipeline_initial = Pipeline([\n",
    "    (\'pre\', preprocessor_initial),\n",
    "    (\'clf\', RandomForestClassifier(class_weight=\'balanced\', random_state=42))\n",
    "]) \n",
    "rf_pipeline_initial.fit(X_train_split_initial, y_train_split_initial)\n",
    "y_pred_rf_initial = rf_pipeline_initial.predict(X_val_initial)\n",
    "y_proba_rf_initial = rf_pipeline_initial.predict_proba(X_val_initial)[:, 1]\n",
    "print(\"\\n--- Random Forest (Inicial) ---\")\n",
    "print(f\"Acurácia validação: {rf_pipeline_initial.score(X_val_initial, y_val_initial):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val_initial, y_pred_rf_initial, digits=4))\n",
    "print(f\"ROC-AUC validação: {roc_auc_score(y_val_initial, y_proba_rf_initial):.4f}\")\n",
    "\n",
    "# XGBoost com RandomizedSearchCV\n",
    "xgb_pipeline_initial = Pipeline([\n",
    "    (\'pre\', preprocessor_initial),\n",
    "    (\'clf\', XGBClassifier(\n",
    "        random_state=42, \n",
    "        eval_metric=\'logloss\',\n",
    "        scale_pos_weight=(y_train_split_initial.value_counts()[0]/y_train_split_initial.value_counts()[1])\n",
    "    ))\n",
    "]) \n",
    "\n",
    "param_dist_initial = {\n",
    "    \'clf__n_estimators\': [100, 200, 300, 400, 500],\n",
    "    \'clf__max_depth\': [2, 3, 4, 5, 6, 7, 8],\n",
    "    \'clf__learning_rate\': [0.005, 0.01, 0.05, 0.075, 0.1, 0.2, 0.3],\n",
    "    \'clf__subsample\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'clf__colsample_bytree\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'clf__gamma\': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \'clf__reg_alpha\': [0, 0.01, 0.1, 1, 10],\n",
    "    \'clf__reg_lambda\': [0.01, 0.1, 1, 5, 10]\n",
    "}\n",
    "\n",
    "random_xgb_initial = RandomizedSearchCV(\n",
    "    xgb_pipeline_initial,\n",
    "    param_dist_initial,\n",
    "    n_iter=20, # Reduzido para exemplo\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_xgb_initial.fit(X_train_split_initial, y_train_split_initial)\n",
    "\n",
    "print(\"\\n--- XGBoost com RandomizedSearch (Inicial) ---\")\n",
    "print(\'Melhor ROC-AUC (cv):\', random_xgb_initial.best_score_)\n",
    "print(\'Melhores parâmetros:\', random_xgb_initial.best_params_)\n",
    "\n",
    "best_rnd_xgb_initial = random_xgb_initial.best_estimator_\n",
    "y_pred_best_rndxgb_initial = best_rnd_xgb_initial.predict(X_val_initial)\n",
    "y_proba_best_rndxgb_initial = best_rnd_xgb_initial.predict_proba(X_val_initial)[:, 1]\n",
    "print(f\"Acurácia validação: {best_rnd_xgb_initial.score(X_val_initial, y_val_initial):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val_initial, y_pred_best_rndxgb_initial, digits=4))\n",
    "print(f\"ROC-AUC validação: {roc_auc_score(y_val_initial, y_proba_best_rndxgb_initial):.4f}\")\n",
    "\n",
    "# Otimização de Threshold\n",
    "thresholds_initial = np.arange(0.3, 0.7, 0.01)\n",
    "f1s_initial = []\n",
    "for th in thresholds_initial:\n",
    "    preds_th = (y_proba_best_rndxgb_initial > th).astype(int)\n",
    "    f1 = f1_score(y_val_initial, preds_th)\n",
    "    f1s_initial.append(f1)\n",
    "best_f1_idx_initial = np.argmax(f1s_initial)\n",
    "best_th_initial = thresholds_initial[best_f1_idx_initial]\n",
    "print(f\"Melhor threshold para submissão (Inicial): {best_th_initial:.3f} (F1 = {f1s_initial[best_f1_idx_initial]:.4f})\")\n",
    "\n",
    "# Geração de submissão (exemplo)\n",
    "# X_test_processed_initial = preprocessor_initial.transform(X_test_initial)\n",
    "# final_proba_test_initial = best_rnd_xgb_initial.predict_proba(X_test_processed_initial)[:,1]\n",
    "# final_preds_initial = (final_proba_test_initial > best_th_initial).astype(int)\n",
    "# submission_initial = pd.DataFrame({\'id\': test[\'id\'], \'labels\': final_preds_initial})\n",
    "# submission_initial.to_csv(\'submission_initial.csv\', index=False)\n",
    "```\n",
    "\n",
    "### 13.2. Abordagens com Target Encoding e Feature Selection (Baseado em `03.py`)\n",
    "\n",
    "Esta iteração explorou o uso de `TargetEncoder` para a variável `category_code` e uma seleção de features baseada na importância de um modelo XGBoost (`SelectFromModel`).\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração com Target Encoding e Feature Selection (03.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "# Engenharia de Features (simplificada para este exemplo, focando nas do 01.py)\n",
    "X_te = train.drop([\'id\', \'labels\'], axis=1).copy()\n",
    "y_te = train[\'labels\']\n",
    "X_test_te = test.drop([\'id\'], axis=1).copy()\n",
    "\n",
    "for df in [X_te, X_test_te]:\n",
    "    df[\'funding_per_round\'] = df[\'funding_total_usd\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'milestones_per_year\'] = df[\'milestones\'] / (df[\'age_last_milestone_year\'].fillna(df[\'age_last_milestone_year\'].median()) + 1)\n",
    "    df[\'funding_per_year\'] = df[\'funding_total_usd\'] / (df[\'age_last_funding_year\'].fillna(df[\'age_last_funding_year\'].median()) + 1)\n",
    "    df[\'participants_per_round\'] = df[\'avg_participants\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'relationships_per_round\'] = df[\'relationships\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'is_CA_software\'] = df[\'is_CA\'] * df[\'is_software\']\n",
    "    df[\'is_NY_advertising\'] = df[\'is_NY\'] * df[\'is_advertising\']\n",
    "    df[\'is_MA_biotech\'] = df[\'is_MA\'] * df[\'is_biotech\']\n",
    "    df[\'is_high_funding\'] = (df[\'funding_total_usd\'] > df[\'funding_total_usd\'].median()).astype(int)\n",
    "    df[\'is_high_milestones\'] = (df[\'milestones\'] > df[\'milestones\'].median()).astype(int)\n",
    "    df[\'time_to_first_milestone_after_funding\'] = (\n",
    "        df[\'age_first_milestone_year\'] - df[\'age_first_funding_year\']\n",
    "    )\n",
    "    df[\'milestone_per_million\'] = df[\'milestones\'] / (df[\'funding_total_usd\'] / 1e6 + 1)\n",
    "\n",
    "cont_features_te = [\n",
    "    \'age_first_funding_year\', \'age_last_funding_year\',\n",
    "    \'age_first_milestone_year\', \'age_last_milestone_year\',\n",
    "    \'relationships\', \'funding_rounds\', \'funding_total_usd\',\n",
    "    \'milestones\', \'avg_participants\', \'relationships_per_round\',\n",
    "    \'participants_per_round\', \'funding_per_year\', \'milestones_per_year\',\n",
    "    \'funding_per_round\', \'time_to_first_milestone_after_funding\', \'milestone_per_million\'
",
    "]\n",
    "bin_features_te = [col for col in X_te.columns if col.startswith(\'is_\') or col.startswith(\'has_\') and col != \'is_othercategory\']\n",
    "cat_features_te = [\'category_code\']\n",
    "\n",
    "all_features_te = cont_features_te + bin_features_te + cat_features_te\n",
    "X_te_processed = X_te[all_features_te].copy()\n",
    "X_test_te_processed = X_test_te[all_features_te].copy()\n",
    "\n",
    "# Target Encoding\n",
    "target_encoder_te = ce.TargetEncoder(cols=[\'category_code\'])\n",
    "X_te_processed[\'category_code\'] = target_encoder_te.fit_transform(X_te_processed[\'category_code\'], y_te)\n",
    "X_test_te_processed[\'category_code\'] = target_encoder_te.transform(X_test_te_processed[\'category_code\'])\n",
    "\n",
    "# Pré-processamento (Imputação e Escalonamento)\n",
    "cont_transformer_te = Pipeline([\n",
    "    (\'imputer\', SimpleImputer(strategy=\'median\')),\n",
    "    (\'scaler\', StandardScaler())\n",
    "]) \n",
    "preprocessor_te = ColumnTransformer([\n",
    "    (\'cont\', cont_transformer_te, cont_features_te),\n",
    "    (\'bin\', \'passthrough\', bin_features_te),\n",
    "    (\'cat\', \'passthrough\', [\'category_code\']) # Passa a coluna já encodada\n",
    "]) \n",
    "\n",
    "X_prep_te = preprocessor_te.fit_transform(X_te_processed)\n",
    "X_test_prep_te = preprocessor_te.transform(X_test_te_processed)\n",
    "\n",
    "# Feature Selection com XGBoost\n",
    "feature_selector_clf_te = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\'logloss\',\n",
    "    scale_pos_weight=(y_te.value_counts()[0]/y_te.value_counts()[1])\n",
    ")\n",
    "feature_selector_clf_te.fit(X_prep_te, y_te)\n",
    "selector_te = SelectFromModel(feature_selector_clf_te, prefit=True, threshold=\'median\')\n",
    "X_fs_te = selector_te.transform(X_prep_te)\n",
    "X_test_fs_te = selector_te.transform(X_test_prep_te)\n",
    "\n",
    "print(f\"\\nNº features selecionadas (Target Encoding + FS): {X_fs_te.shape[1]}/{X_prep_te.shape[1]}\")\n",
    "\n",
    "X_train_split_te, X_val_te, y_train_split_te, y_val_te = train_test_split(X_fs_te, y_te, stratify=y_te, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost com RandomizedSearchCV (após FS)\n",
    "param_dist_te = {\n",
    "    \'n_estimators\': [100, 200, 300, 400, 500],\n",
    "    \'max_depth\': [2, 3, 4, 5, 6, 7],\n",
    "    \'learning_rate\': [0.005, 0.01, 0.05, 0.075, 0.1, 0.2, 0.3],\n",
    "    \'subsample\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'colsample_bytree\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'gamma\': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \'reg_alpha\': [0, 0.01, 0.1, 1, 10],\n",
    "    \'reg_lambda\': [0.01, 0.1, 1, 5, 10]\n",
    "}\n",
    "random_xgb_te = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric=\'logloss\',\n",
    "                  scale_pos_weight=(y_train_split_te.value_counts()[0]/y_train_split_te.value_counts()[1])),\n",
    "    param_dist_te,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_xgb_te.fit(X_train_split_te, y_train_split_te)\n",
    "best_xgb_te = random_xgb_te.best_estimator_\n",
    "\n",
    "print(\"\\n--- XGBoost RANDOMIZED SEARCH (Target Encoding + FS) ---\")\n",
    "print(\'Melhores parâmetros:\', random_xgb_te.best_params_)\n",
    "print(f\"Acurácia validação: {best_xgb_te.score(X_val_te, y_val_te):.3f}\")\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_val_te, best_xgb_te.predict(X_val_te)))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val_te, best_xgb_te.predict_proba(X_val_te)[:,1]):.4f}\")\n",
    "\n",
    "# Otimização de Threshold\n",
    "thresholds_te = np.arange(0.2, 0.7, 0.005)\n",
    "f1s_te = []\n",
    "y_proba_val_te = best_xgb_te.predict_proba(X_val_te)[:,1]\n",
    "for th in thresholds_te:\n",
    "    preds_th = (y_proba_val_te > th).astype(int)\n",
    "    f1 = f1_score(y_val_te, preds_th)\n",
    "    f1s_te.append(f1)\n",
    "best_f1_idx_te = np.argmax(f1s_te)\n",
    "best_th_te = thresholds_te[best_f1_idx_te]\n",
    "print(f\"Melhor threshold para submissão (Target Encoding + FS): {best_th_te:.3f} (F1 = {f1s_te[best_f1_idx_te]:.4f})\")\n",
    "\n",
    "# Geração de submissão (exemplo)\n",
    "# final_proba_test_te = best_xgb_te.predict_proba(X_test_fs_te)[:,1]\n",
    "# final_preds_te = (final_proba_test_te > best_th_te).astype(int)\n",
    "# submission_te = pd.DataFrame({\'id\': test[\'id\'], \'labels\': final_preds_te})\n",
    "# submission_te.to_csv(\'submission_te.csv\', index=False)\n",
    "```\n",
    "\n",
    "### 13.3. Abordagem com Feature Engineering e Análise Exploratória (Baseado em `04.py`)\n",
    "\n",
    "Esta iteração focou em uma engenharia de features mais elaborada e em uma análise exploratória detalhada, incluindo visualizações e identificação de multicolinearidade. A variável `category_code` foi removida, e a seleção de features foi manual, baseada nas features engenheiradas.\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração com Feature Engineering e Análise Exploratória (04.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "def engineer_features_04(df):\n",
    "  df_enhanced = df.copy()\n",
    "  df_enhanced[\'milestones_per_funding_round\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "  df_enhanced[\'avg_funding_per_round\'] = df_enhanced[\'funding_total_usd\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "  df_enhanced[\'relationships_per_milestone\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'milestones\'] + 0.1)\n",
    "  df_enhanced[\'time_to_first_funding\'] = df_enhanced[\'age_first_funding_year\'].fillna(999)\n",
    "  df_enhanced[\'funding_duration\'] = (df_enhanced[\'age_last_funding_year\'] - df_enhanced[\'age_first_funding_year\']).fillna(0)\n",
    "  df_enhanced[\'time_to_first_milestone\'] = df_enhanced[\'age_first_milestone_year\'].fillna(999)\n",
    "  df_enhanced[\'milestone_duration\'] = (df_enhanced[\'age_last_milestone_year\'] - df_enhanced[\'age_first_milestone_year\']).fillna(0)\n",
    "  df_enhanced[\'funding_progression_score\'] = (\n",
    "      df_enhanced[\'has_roundA\'] * 1 +\n",
    "      df_enhanced[\'has_roundB\'] * 2 +\n",
    "      df_enhanced[\'has_roundC\'] * 3 +\n",
    "      df_enhanced[\'has_roundD\'] * 4\n",
    "  )\n",
    "  df_enhanced[\'funding_diversity\'] = df_enhanced[\'has_VC\'] + df_enhanced[\'has_angel\']\n",
    "  df_enhanced[\'has_later_stage\'] = ((df_enhanced[\'has_roundC\'] == 1) | (df_enhanced[\'has_roundD\'] == 1)).astype(int)\n",
    "  df_enhanced[\'location_success_score\'] = (\n",
    "      df_enhanced[\'is_MA\'] * 0.82 +\n",
    "      df_enhanced[\'is_NY\'] * 0.70 +\n",
    "      df_enhanced[\'is_CA\'] * 0.69 +\n",
    "      df_enhanced[\'is_TX\'] * 0.46 +\n",
    "      df_enhanced[\'is_otherstate\'] * 0.46\n",
    "  )\n",
    "  df_enhanced[\'is_top_tier_location\'] = ((df_enhanced[\'is_CA\'] == 1) | \n",
    "                                        (df_enhanced[\'is_NY\'] == 1) | \n",
    "                                        (df_enhanced[\'is_MA\'] == 1)).astype(int)\n",
    "  df_enhanced[\'sector_success_score\'] = (\n",
    "      df_enhanced[\'is_enterprise\'] * 0.75 +\n",
    "      df_enhanced[\'is_advertising\'] * 0.69 +\n",
    "      df_enhanced[\'is_web\'] * 0.68 +\n",
    "      df_enhanced[\'is_biotech\'] * 0.68 +\n",
    "      df_enhanced[\'is_mobile\'] * 0.66 +\n",
    "      df_enhanced[\'is_software\'] * 0.64 +\n",
    "      df_enhanced[\'is_gamesvideo\'] * 0.62 +\n",
    "      df_enhanced[\'is_othercategory\'] * 0.62 +\n",
    "      df_enhanced[\'is_consulting\'] * 0.50 +\n",
    "      df_enhanced[\'is_ecommerce\'] * 0.40\n",
    "  )\n",
    "  df_enhanced[\'is_high_success_sector\'] = ((df_enhanced[\'is_enterprise\'] == 1) | \n",
    "                                          (df_enhanced[\'is_advertising\'] == 1) | \n",
    "                                          (df_enhanced[\'is_web\'] == 1) | \n",
    "                                          (df_enhanced[\'is_biotech\'] == 1)).astype(int)\n",
    "  df_enhanced[\'total_activity_score\'] = (\n",
    "      df_enhanced[\'relationships\'] + \n",
    "      df_enhanced[\'milestones\'] + \n",
    "      df_enhanced[\'funding_rounds\']\n",
    "  )\n",
    "  df_enhanced[\'funding_efficiency\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'funding_total_usd\'] / 1000000 + 0.1)\n",
    "  df_enhanced[\'is_mature_startup\'] = ((df_enhanced[\'milestones\'] >= 2) & \n",
    "                                      (df_enhanced[\'funding_rounds\'] >= 2)).astype(int)\n",
    "  df_enhanced[\'location_tech_interaction\'] = (\n",
    "      df_enhanced[\'is_top_tier_location\'] * \n",
    "      (df_enhanced[\'is_software\'] + df_enhanced[\'is_web\'] + df_enhanced[\'is_mobile\'])\n",
    "  )\n",
    "  df_enhanced[\'funding_relationships_interaction\'] = (\n",
    "      df_enhanced[\'funding_progression_score\'] * df_enhanced[\'relationships\']\n",
    "  )\n",
    "  df_enhanced[\'maturity_funding_interaction\'] = (\n",
    "      df_enhanced[\'is_mature_startup\'] * np.log1p(df_enhanced[\'funding_total_usd\'])\n",
    "  )\n",
    "  df_enhanced[\'has_recent_milestones\'] = (\n",
    "      (df_enhanced[\'age_last_milestone_year\'].fillna(999) <= 3)\n",
    "  ).astype(int)\n",
    "  return df_enhanced\n",
    "\n",
    "# Remove colunas não usadas\n",
    "train_04 = train.drop(columns=[\'id\', \'category_code\']) # Assumindo que train é o df original\n",
    "test_04 = test.drop(columns=[\'id\', \'category_code\']) # Assumindo que test é o df original\n",
    "\n",
    "# Feature Engineering\n",
    "X_train_enhanced_04 = engineer_features_04(train_04.drop(columns=[\'labels\']))\n",
    "X_test_enhanced_04 = engineer_features_04(test_04)\n",
    "y_train_04 = train_04[\'labels\']\n",
    "\n",
    "# Tratamento de missing (imputação por mediana)\n",
    "for col in X_train_enhanced_04.columns:\n",
    "    if X_train_enhanced_04[col].dtype in [np.float64, np.int64]:\n",
    "        mediana = X_train_enhanced_04[col].median()\n",
    "        X_train_enhanced_04.fillna({col: mediana}, inplace=True)\n",
    "        X_test_enhanced_04.fillna({col: mediana}, inplace=True)\n",
    "\n",
    "# Tratamento de outliers (clipping nos percentis extremos)\n",
    "for col in X_train_enhanced_04.columns:\n",
    "    if X_train_enhanced_04[col].dtype in [np.float64, np.int64]:\n",
    "        lower = X_train_enhanced_04[col].quantile(0.25)\n",
    "        upper = X_train_enhanced_04[col].quantile(0.75)\n",
    "        X_train_enhanced_04[col] = X_train_enhanced_04[col].clip(lower, upper)\n",
    "        X_test_enhanced_04[col] = X_test_enhanced_04[col].clip(lower, upper)\n",
    "\n",
    "# Seleção de features manual (exemplo)\n",
    "engineered_feature_names_04 = [\n",
    "    \'total_activity_score\', \'maturity_funding_interaction\', \'funding_progression_score\',\n",
    "    \'is_mature_startup\', \'time_to_first_milestone\', \'funding_relationships_interaction\',\n",
    "    \'milestone_duration\', \'location_success_score\', \'funding_duration\',\n",
    "    \'is_top_tier_location\', \'has_later_stage\', \'milestones_per_funding_round\',\n",
    "    \'funding_efficiency\'
",
    "]\n",
    "\n",
    "X_train_selected_04 = X_train_enhanced_04[engineered_feature_names_04]\n",
    "X_test_selected_04 = X_test_enhanced_04[engineered_feature_names_04]\n",
    "\n",
    "X_train_split_04, X_val_04, y_train_split_04, y_val_04 = train_test_split(\n",
    "  X_train_selected_04, y_train_04, test_size=0.2, stratify=y_train_04, random_state=42\n",
    ")\n",
    "\n",
    "# Modelagem com Random Forest e XGBoost\n",
    "models_04 = {\n",
    "  \'RandomForest\': RandomForestClassifier(class_weight=\'balanced\', n_estimators=100, random_state=42),\n",
    "  \'XGBClassifier\': XGBClassifier(class_weight=\'balanced\', n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model_04 in models_04.items():\n",
    "    model_04.fit(X_train_split_04, y_train_split_04)\n",
    "    y_pred_04 = model_04.predict(X_val_04)\n",
    "    y_prob_04 = model_04.predict_proba(X_val_04)[:, 1]\n",
    "    print(f\"\\nModelo: {name} (04.py)\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_val_04, y_prob_04):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_val_04, y_pred_04):.4f}\")\n",
    "    print(classification_report(y_val_04, y_pred_04))\n",
    "\n",
    "# Geração de submissão (exemplo)\n",
    "# test_pred_prob_04 = models_04[\'XGBClassifier\'].predict_proba(X_test_selected_04)[:, 1]\n",
    "# optimal_threshold_04 = 0.5 # Assumindo threshold padrão para exemplo\n",
    "# test_pred_label_04 = (test_pred_prob_04 >= optimal_threshold_04).astype(int)\n",
    "# submission_04 = pd.DataFrame({\'id\': test[\'id\'], \'labels\': test_pred_label_04})\n",
    "# submission_04.to_csv(\'submission_04.csv\', index=False)\n",
    "```\n"
   ]
  }
]

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.2. Engenharia de Features Avançada e Otimização de Threshold (Baseado em `02.py`)\n",
    "\n",
    "Esta iteração aprofundou a engenharia de features, introduzindo variáveis mais complexas baseadas em pesquisa, e focou na otimização do *threshold* de classificação para maximizar o F1-score, uma métrica mais adequada para datasets desbalanceados. Também incluiu a visualização da importância das features.\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração com Feature Engineering Avançada e Otimização de Threshold (02.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "# Para garantir que as variáveis `train` e `test` estejam disponíveis para este bloco de código, \n",
    "# vamos recriá-las aqui, caso o notebook seja executado em partes.\n",
    "# Lembre-se de que, em um notebook real, você carregaria os dados uma única vez no início.\n",
    "try:\n",
    "    train = pd.read_csv(\'train.csv\')\n",
    "    test = pd.read_csv(\'test.csv\')\n",
    "except FileNotFoundError:\n",
    "    print(\"Certifique-se de que \'train.csv\' e \'test.csv\' estão no diretório correto.\")\n",
    "    # Criar DataFrames vazios ou mock para evitar erros se os arquivos não existirem\n",
    "    train = pd.DataFrame(columns=[\"id\", \"labels\"] + [f\"col{i}\" for i in range(30)])\n",
    "    test = pd.DataFrame(columns=[\"id\"] + [f\"col{i}\" for i in range(30)])\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Features originais e \"clássicas\"\n",
    "    df[\\'funding_per_round\\'] = df[\\'funding_total_usd\\'] / (df[\\'funding_rounds\\'] + 1e-9)\n",
    "    df[\\'milestones_per_year\\'] = df[\\'milestones\\'] / (df[\\'age_last_milestone_year\\'].fillna(df[\\'age_last_milestone_year\\'].median()) + 1)\n",
    "    df[\\'funding_per_year\\'] = df[\\'funding_total_usd\\'] / (df[\\'age_last_funding_year\\'].fillna(df[\\'age_last_funding_year\\'].median()) + 1)\n",
    "    df[\\'participants_per_round\\'] = df[\\'avg_participants\\'] / (df[\\'funding_rounds\\'] + 1e-9)\n",
    "    df[\\'relationships_per_round\\'] = df[\\'relationships\\'] / (df[\\'funding_rounds\\'] + 1e-9)\n",
    "    df[\\'is_CA_software\\'] = df[\\'is_CA\\'] * df[\\'is_software\\']\n",
    "    df[\\'is_NY_advertising\\'] = df[\\'is_NY\\'] * df[\\'is_advertising\\']\n",
    "    df[\\'is_MA_biotech\\'] = df[\\'is_MA\\'] * df[\\'is_biotech\\']\n",
    "    df[\\'is_high_funding\\'] = (df[\\'funding_total_usd\\'] > df[\\'funding_total_usd\\'].median()).astype(int)\n",
    "    df[\\'is_high_milestones\\'] = (df[\\'milestones\\'] > df[\\'milestones\\'].median()).astype(int)\n",
    "    df[\\'time_to_first_milestone_after_funding\\'] = (\n",
    "        df[\\'age_first_milestone_year\\'] - df[\\'age_first_funding_year\\']\n",
    "    )\n",
    "    df[\\'milestone_per_million\\'] = df[\\'milestones\\'] / (df[\\'funding_total_usd\\'] / 1e6 + 1)\n",
    "    \n",
    "    # --- Features avançadas de pesquisa em sucesso de startups ----\n",
    "    df[\\'has_multiple_funding_types\\'] = ((df[\\'has_VC\\'] + df[\\'has_angel\\']) >= 2).astype(int)\n",
    "    df[\\'advanced_funding_rounds\\'] = ((df[\\'has_roundC\\'] + df[\\'has_roundD\\']) >= 1).astype(int)\n",
    "    df[\\'funding_progression_score\\'] = (df[\\'has_roundA\\'] + df[\\'has_roundB\\'] + df[\\'has_roundC\\'] + df[\\'has_roundD\\'])\n",
    "    df[\\'funding_velocity\\'] = df[\\'funding_total_usd\\'] / (df[\\'age_last_funding_year\\'].fillna(1) + 1)\n",
    "    df[\\'milestone_momentum\\'] = df[\\'milestones\\'] / (df[\\'age_last_milestone_year\\'].fillna(1) + 1)\n",
    "    df[\\'network_density\\'] = df[\\'relationships\\'] * df[\\'avg_participants\\'] / (df[\\'funding_rounds\\'] + 1)\n",
    "    df[\\'ecosystem_strength\\'] = df[\\'relationships\\'] + df[\\'milestones\\'] + df[\\'funding_rounds\\']\n",
    "    df[\\'traction_score\\'] = (df[\\'funding_total_usd\\'] / 1000000) * df[\\'milestones\\'] * df[\\'relationships\\']\n",
    "    df[\\'is_high_traction\\'] = (df[\\'traction_score\\'] > df[\\'traction_score\\'].quantile(0.75)).astype(int)\n",
    "    df[\\'efficiency_ratio\\'] = df[\\'milestones\\'] / (df[\\'funding_total_usd\\'] / 1000000 + 1)\n",
    "    df[\\'capital_efficiency\\'] = df[\\'relationships\\'] / (df[\\'funding_total_usd\\'] / 1000000 + 1)\n",
    "    df[\\'funding_milestone_alignment\\'] = np.abs(\n",
    "        df[\\'age_last_funding_year\\'].fillna(0) - df[\\'age_last_milestone_year\\'].fillna(0)\n",
    "    )\n",
    "    df[\\'is_well_timed_funding\\'] = (df[\\'funding_milestone_alignment\\'] <= 1).astype(int)\n",
    "    df[\\'is_high_growth_sector\\'] = (\n",
    "        df[\\'is_software\\'] + df[\\'is_biotech\\'] + df[\\'is_mobile\\'] + df[\\'is_enterprise\\']\n",
    "    ).astype(int)\n",
    "    df[\\'is_traditional_sector\\'] = (\n",
    "        df[\\'is_consulting\\'] + df[\\'is_advertising\\']\n",
    "    ).astype(int)\n",
    "    df[\\'is_innovation_hub\\'] = (df[\\'is_CA\\'] + df[\\'is_NY\\'] + df[\\'is_MA\\']).astype(int)\n",
    "    df[\\'innovation_hub_tech\\'] = df[\\'is_innovation_hub\\'] * df[\\'is_high_growth_sector\\']\n",
    "    df[\\'scale_readiness\\'] = (\n",
    "        (df[\\'funding_rounds\\'] >= 3).astype(int) + \n",
    "        (df[\\'relationships\\'] >= df[\\'relationships\\'].median()).astype(int) +\n",
    "        (df[\\'milestones\\'] >= df[\\'milestones\\'].median()).astype(int)\n",
    "    )\n",
    "    df[\\'diversified_funding\\'] = df[\\'funding_progression_score\\'] * df[\\'has_multiple_funding_types\\']\n",
    "    df[\\'sustainable_growth_pattern\\'] = (\n",
    "        df[\\'is_well_timed_funding\\'] * df[\\'efficiency_ratio\\'] * df[\\'network_density\\']\n",
    "    )\n",
    "    df[\\'competitive_moat\\'] = df[\\'milestones\\'] * df[\\'relationships\\'] / (df[\\'funding_rounds\\'] + 1)\n",
    "    df[\\'market_position_strength\\'] = (\n",
    "        df[\\'funding_total_usd\\'] / (df[\\'funding_total_usd\\'].median() + 1)\n",
    "    ) * df[\\'milestone_momentum\\']\n",
    "\n",
    "# Atualização das listas de features\n",
    "cont_features = [\n",
    "    \\'age_first_funding_year\\', \\'age_last_funding_year\\',\n",
    "    \\'age_first_milestone_year\\', \\'age_last_milestone_year\\',\n",
    "    \\'relationships\\', \\'funding_rounds\\', \\'funding_total_usd\\',\n",
    "    \\'milestones\\', \\'avg_participants\\', \\'relationships_per_round\\',\n",
    "    \\'participants_per_round\\', \\'funding_per_year\\', \\'milestones_per_year\\',\n",
    "    \\'funding_per_round\\', \\'time_to_first_milestone_after_funding\\',\n",
    "    # Avançadas:\n",
    "    \\'funding_velocity\\', \\'milestone_momentum\\', \\'network_density\\', \\'ecosystem_strength\\',\n",
    "    \\'traction_score\\', \\'efficiency_ratio\\', \\'capital_efficiency\\', \\'funding_milestone_alignment\\',\n",
    "    \\'competitive_moat\\', \\'market_position_strength\\', \n",
    "    \\'funding_progression_score\\', \\'scale_readiness\\', \\'diversified_funding\\', \\'sustainable_growth_pattern\\'\n",
    "]\n",
    "cat_features = [\\'category_code\\']\n",
    "bin_features = [col for col in train.columns if col.startswith(\'is_\') or col.startswith(\'has_\') and col != \'is_othercategory\']\n",
    "bin_features += [\n",
    "    \\'has_multiple_funding_types\\', \\'advanced_funding_rounds\\', \\'is_high_traction\\',\n",
    "    \\'is_well_timed_funding\\', \\'is_high_growth_sector\\', \\'is_traditional_sector\\',\n",
    "    \\'is_innovation_hub\\', \\'innovation_hub_tech\\'\n",
    "]\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "cont_transformer = Pipeline([\n",
    "    (\'imputer\', SimpleImputer(strategy=\'median\')),\n",
    "    (\'scaler\', StandardScaler())\n",
    "]) \n",
    "cat_transformer = Pipeline([\n",
    "    (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))\n",
    "]) \n",
    "preprocessor = ColumnTransformer([\n",
    "    (\'cont\', cont_transformer, cont_features),\n",
    "    (\'cat\', cat_transformer, cat_features),\n",
    "    (\'bin\', \'passthrough\', bin_features)\n",
    "]) \n",
    "\n",
    "X = train.drop([\\'id\\', \\'labels\\'], axis=1)\n",
    "y = train[\\'labels\\']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n========== RANDOM FOREST ==========\")\n",
    "model = Pipeline([\n",
    "    (\'pre\', preprocessor),\n",
    "    (\'clf\', RandomForestClassifier(class_weight=\'balanced\', random_state=42))\n",
    "]) \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "rf_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "print(f\"Acurácia validação: {model.score(X_val, y_val):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val, y_pred, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(f\"ROC-AUC validação: {rf_auc:.4f}\")\n",
    "\n",
    "# ===== Validação cruzada RF =====\n",
    "rf_scores = cross_val_score(model, X, y, \n",
    "                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                            scoring=\'roc_auc\')\n",
    "print(f\"Validação cruzada RandomForest ROC-AUC: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n========== XGBOOST (Default) ==========\")\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\'pre\', preprocessor),\n",
    "    (\'clf\', XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\'logloss\',\n",
    "        scale_pos_weight=(y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "    ))\n",
    "]) \n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_val)\n",
    "y_proba_xgb = xgb_pipeline.predict_proba(X_val)[:, 1]\n",
    "xgb_auc = roc_auc_score(y_val, y_proba_xgb)\n",
    "print(f\"Acurácia validação: {xgb_pipeline.score(X_val, y_val):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val, y_pred_xgb, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_xgb))\n",
    "print(f\"ROC-AUC validação: {xgb_auc:.4f}\")\n",
    "\n",
    "# ===== Validação cruzada XGB default =====\n",
    "xgb_scores = cross_val_score(xgb_pipeline, X, y, \n",
    "                             cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                             scoring=\'roc_auc\')\n",
    "print(f\"Validação cruzada XGBoost padrão ROC-AUC: {xgb_scores.mean():.4f} ± {xgb_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n========== XGBOOST (GridSearchCV) ==========\")\n",
    "param_grid = {\n",
    "    \\'clf__n_estimators\\': [100, 200],\n",
    "    \\'clf__max_depth\\': [3, 5, 7],\n",
    "    \\'clf__learning_rate\\': [0.01, 0.1, 0.2],\n",
    "    \\'clf__subsample\\': [0.8, 1.0]\n",
    "}\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "y_pred_bestxgb = best_xgb.predict(X_val)\n",
    "y_proba_bestxgb = best_xgb.predict_proba(X_val)[:, 1]\n",
    "best_xgb_auc = roc_auc_score(y_val, y_proba_bestxgb)\n",
    "print(f\"Melhor ROC-AUC (cv): {grid_xgb.best_score_:.4f}\")\n",
    "print(f\"Melhores parâmetros: {grid_xgb.best_params_}\")\n",
    "print(f\"Acurácia validação: {best_xgb.score(X_val, y_val):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val, y_pred_bestxgb, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_bestxgb))\n",
    "print(f\"ROC-AUC validação: {best_xgb_auc:.4f}\")\n",
    "\n",
    "# ===== Validação cruzada XGB GridSearch =====\n",
    "best_xgb_scores = cross_val_score(best_xgb, X, y, \n",
    "                                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                  scoring=\'roc_auc\')\n",
    "print(f\"Validação cruzada XGBoost GridSearchCV ROC-AUC: {best_xgb_scores.mean():.4f} ± {best_xgb_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n========== XGBOOST (RandomizedSearchCV) ==========\")\n",
    "param_dist = {\n",
    "    \\'clf__n_estimators\\': [100, 200, 300, 400, 500],\n",
    "    \\'clf__max_depth\\': [2, 3, 4, 5, 6, 7, 8],\n",
    "    \\'clf__learning_rate\\': [0.005, 0.01, 0.05, 0.075, 0.1, 0.2, 0.3],\n",
    "    \\'clf__subsample\\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \\'clf__colsample_bytree\\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \\'clf__gamma\\': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \\'clf__reg_alpha\\': [0, 0.01, 0.1, 1, 10],\n",
    "    \\'clf__reg_lambda\\': [0.01, 0.1, 1, 5, 10]\n",
    "}\n",
    "random_xgb = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_dist,\n",
    "    n_iter=40,\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_xgb.fit(X_train, y_train)\n",
    "print(f\"Melhor ROC-AUC (cv): {random_xgb.best_score_:.4f}\")\n",
    "print(f\"Melhores parâmetros: {random_xgb.best_params_}\")\n",
    "best_rnd_xgb = random_xgb.best_estimator_\n",
    "y_pred_best_rndxgb = best_rnd_xgb.predict(X_val)\n",
    "y_proba_best_rndxgb = best_rnd_xgb.predict_proba(X_val)[:, 1]\n",
    "best_rnd_xgb_auc = roc_auc_score(y_val, y_proba_best_rndxgb)\n",
    "print(f\"Acurácia validação: {best_rnd_xgb.score(X_val, y_val):.4f}\")\n",
    "print(\"Classif. Report:\\n\", classification_report(y_val, y_pred_best_rndxgb, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_best_rndxgb))\n",
    "print(f\"ROC-AUC validação: {best_rnd_xgb_auc:.4f}\")\n",
    "\n",
    "# ===== Validação cruzada XGB RandomizedSearch =====\n",
    "best_rnd_xgb_scores = cross_val_score(best_rnd_xgb, X, y, \n",
    "                                      cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                      scoring=\'roc_auc\')\n",
    "print(f\"Validação cruzada XGBoost RandomizedSearchCV ROC-AUC: {best_rnd_xgb_scores.mean():.4f} ± {best_rnd_xgb_scores.std():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n========== F1 THRESHOLD TUNING (RandomizedSearchCV) ==========\")\n",
    "thresholds = np.arange(0.3, 0.7, 0.01)\n",
    "best_f1 = 0\n",
    "best_th = 0.5\n",
    "for th in thresholds:\n",
    "    preds = (y_proba_best_rndxgb > th).astype(int)\n",
    "    score = f1_score(y_val, preds)\n",
    "    print(f\"Threshold {th:.2f} -> F1: {score:.4f}\")\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_th = th\n",
    "print(f\"\\033[1mMelhor threshold para submissão (RandomizedSearchCV): {best_th:.2f} (F1={best_f1:.4f})\\033[0m\")\n",
    "\n",
    "print(\"\\n========== F1 THRESHOLD TUNING (GridSearchCV) ==========\")\n",
    "thresholds_grid = np.arange(0.3, 0.7, 0.01)\n",
    "best_f1_grid = 0\n",
    "best_th_grid = 0.5\n",
    "for th in thresholds_grid:\n",
    "    preds_grid = (y_proba_bestxgb > th).astype(int)\n",
    "    score_grid = f1_score(y_val, preds_grid)\n",
    "    print(f\"Threshold {th:.2f} -> F1: {score_grid:.4f}\")\n",
    "    if score_grid > best_f1_grid:\n",
    "        best_f1_grid = score_grid\n",
    "        best_th_grid = th\n",
    "print(f\"\\033[1mMelhor threshold para submissão (GridSearchCV): {best_th_grid:.2f} (F1={best_f1_grid:.4f})\\033[0m\")\n",
    "\n",
    "# Exemplo para obter importâncias do melhor modelo XGBoost\n",
    "# best_model = random_xgb.best_estimator_ # Ou grid_xgb.best_estimator_\n",
    "# preprocessor = best_model.named_steps[\\'pre\\']\n",
    "# classifier = best_model.named_steps[\\'clf\\']\n",
    "\n",
    "# # Obter nomes das features após o pré-processamento\n",
    "# # Isso pode ser complexo devido ao OneHotEncoder e ColumnTransformer. \n",
    "# # Uma abordagem simplificada para este exemplo:\n",
    "# # all_feature_names = cont_features + list(preprocessor.named_transformers_[\\'cat\\'].named_steps[\\'onehot\\'].get_feature_names_out(cat_features)) + bin_features\n",
    "# # importances = pd.Series(classifier.feature_importances_, index=all_feature_names)\n",
    "# # top_features = importances.sort_values(ascending=False).head(20)\n",
    "\n",
    "# # Plotar\n",
    "# # plt.figure(figsize=(10, 8))\n",
    "# # sns.barplot(x=top_features, y=top_features.index)\n",
    "# # plt.title(\'Top 20 Feature Importances\')\n",
    "# # plt.show()\n",
    "\n",
    "# Geração de submissão (exemplo com o melhor threshold do GridSearchCV)\n",
    "# X_test_processed = preprocessor.transform(test.drop([\\'id\\'], axis=1))\n",
    "# y_proba_test_final = best_xgb.predict_proba(X_test_processed)[:, 1]\n",
    "# preds_test_final = (y_proba_test_final > best_th_grid).astype(int)\n",
    "# submission_final = pd.DataFrame({\'id\': test[\\'id\\'], \'labels\': preds_test_final})\n",
    "# submission_final.to_csv(\'submission_final_02.csv\', index=False)\n",
    "```"
   ]
  }

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.3. Abordagem Inicial com Feature Engineering Simples (Baseado em `01.py`)\n",
    "\n",
    "Esta foi a primeira iteração, focando em uma engenharia de features mais básica e na aplicação de Random Forest e XGBoost com configurações padrão ou otimização inicial de hiperparâmetros.\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração inicial (01.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "# Para garantir que as variáveis `train` e `test` estejam disponíveis para este bloco de código, \n",
    "# vamos recriá-las aqui, caso o notebook seja executado em partes.\n",
    "# Lembre-se de que, em um notebook real, você carregaria os dados uma única vez no início.\n",
    "try:\n",
    "    train = pd.read_csv(\'train.csv\')\n",
    "    test = pd.read_csv(\'test.csv\')\n",
    "except FileNotFoundError:\n",
    "    print(\"Certifique-se de que \'train.csv\' e \'test.csv\' estão no diretório correto.\")\n",
    "    # Criar DataFrames vazios ou mock para evitar erros se os arquivos não existirem\n",
    "    train = pd.DataFrame(columns=[\"id\", \"labels\"] + [f\"col{i}\" for i in range(30)])\n",
    "    test = pd.DataFrame(columns=[\"id\"] + [f\"col{i}\" for i in range(30)])\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Funding por rodada\n",
    "    df[\'funding_per_round\'] = df[\'funding_total_usd\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    # Milestones por cada ano da idade do último milestone (preenche NaN com mediana)\n",
    "    df[\'milestones_per_year\'] = df[\'milestones\'] / (df[\'age_last_milestone_year\'].fillna(df[\'age_last_milestone_year\'].median()) + 1)\n",
    "    # Funding dividido pela idade de captação final\n",
    "    df[\'funding_per_year\'] = df[\'funding_total_usd\'] / (df[\'age_last_funding_year\'].fillna(df[\'age_last_funding_year\'].median()) + 1)\n",
    "    # Relação de investidores por rodada\n",
    "    df[\'participants_per_round\'] = df[\'avg_participants\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    # Relação de networking por rodada\n",
    "    df[\'relationships_per_round\'] = df[\'relationships\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    # Interações de dummies regionais/setoriais\n",
    "    df[\'is_CA_software\'] = df[\'is_CA\'] * df[\'is_software\']\n",
    "    df[\'is_NY_advertising\'] = df[\'is_NY\'] * df[\'is_advertising\']\n",
    "    df[\'is_MA_biotech\'] = df[\'is_MA\'] * df[\'is_biotech\']\n",
    "    # Feature binária por faixas de funding (>mediana)\n",
    "    df[\'is_high_funding\'] = (df[\'funding_total_usd\'] > df[\'funding_total_usd\'].median()).astype(int)\n",
    "    # Feature binária por faixas de milestones (>mediana)\n",
    "    df[\'is_high_milestones\'] = (df[\'milestones\'] > df[\'milestones\'].median()).astype(int)\n",
    "    df[\'time_to_first_milestone_after_funding\'] = (\n",
    "        df[\'age_first_milestone_year\'] - df[\'age_first_funding_year\']\n",
    "    )\n",
    "    df[\'milestone_per_million\'] = df[\'milestones\'] / (df[\'funding_total_usd\'] / 1e6 + 1)\n",
    "\n",
    "# Separando features\n",
    "cont_features = [\n",
    "    \'age_first_funding_year\', \'age_last_funding_year\',\n",
    "    \'age_first_milestone_year\', \'age_last_milestone_year\',\n",
    "    \'relationships\', \'funding_rounds\', \'funding_total_usd\',\n",
    "    \'milestones\', \'avg_participants\', \'relationships_per_round\',\n",
    "    \'participants_per_round\', \'funding_per_year\', \'milestones_per_year\',\n",
    "    \'funding_per_round\', \'time_to_first_milestone_after_funding\', \'milestone_per_million\'
",
    "]\n",
    "cat_features = [\'category_code\']\n",
    "bin_features = [col for col in train.columns if col.startswith(\'is_\') or col.startswith(\'has_\') and col != \'is_othercategory\']\n",
    "\n",
    "# Imputação + Escalonamento para contínuas\n",
    "cont_transformer = Pipeline([\n",
    "    (\'imputer\', SimpleImputer(strategy=\'median\')),\n",
    "    (\'scaler\', StandardScaler())\n",
    "]) \n",
    "# One-Hot para categórica\n",
    "cat_transformer = Pipeline([\n",
    "    (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))\n",
    "]) \n",
    "preprocessor = ColumnTransformer([\n",
    "    (\'cont\', cont_transformer, cont_features),\n",
    "    (\'cat\', cat_transformer, cat_features),\n",
    "    (\'bin\', \'passthrough\', bin_features)\n",
    "]) \n",
    "\n",
    "X = train.drop([\'id\', \'labels\'], axis=1)\n",
    "y = train[\'labels\']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([\n",
    "    (\'pre\', preprocessor),\n",
    "    (\'clf\', RandomForestClassifier(class_weight=\'balanced\', random_state=42))\n",
    "]) \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_val, y_val)\n",
    "print(f\'Acurácia no conjunto de validação: {score:.3f}\')\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\'ROC-AUC:\', roc_auc_score(y_val, model.predict_proba(X_val)[:,1]))\n",
    "\n",
    "# XGBoost pipeline SEM hiperparâmetros ainda\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\'pre\', preprocessor),\n",
    "    (\'clf\', XGBClassifier(\n",
    "        random_state=42, \n",
    "        eval_metric=\'logloss\',\n",
    "        scale_pos_weight=(y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "    ))\n",
    "]) \n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_val)\n",
    "y_proba_xgb = xgb_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\'--- XGBoost ---\' )\n",
    "print(f\'Acurácia: {xgb_pipeline.score(X_val, y_val):.3f}\')\n",
    "print(classification_report(y_val, y_pred_xgb))\n",
    "print(confusion_matrix(y_val, y_pred_xgb))\n",
    "print(\'ROC-AUC:\', roc_auc_score(y_val, y_proba_xgb))\n",
    "\n",
    "# AJUSTE DE HIPERPARÂMETROS COM GRIDSEARCHCV\n",
    "param_grid = {\n",
    "    \'clf__n_estimators\': [100, 200],\n",
    "    \'clf__max_depth\': [3, 5, 7],\n",
    "    \'clf__learning_rate\': [0.01, 0.1, 0.2],\n",
    "    \'clf__subsample\': [0.8, 1.0]\n",
    "}\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\'--- XGBoost com hiperparâmetros ótimos ---\' )\n",
    "print(\'Melhor ROC-AUC (cv):\', grid_xgb.best_score_)\n",
    "print(\'Melhores parâmetros:\', grid_xgb.best_params_)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "print(f\'Acurácia validação: {best_xgb.score(X_val, y_val):.3f}\')\n",
    "y_pred_bestxgb = best_xgb.predict(X_val)\n",
    "y_proba_bestxgb = best_xgb.predict_proba(X_val)[:, 1]\n",
    "print(classification_report(y_val, y_pred_bestxgb))\n",
    "print(confusion_matrix(y_val, y_pred_bestxgb))\n",
    "print(\'ROC-AUC:\', roc_auc_score(y_val, y_proba_bestxgb))\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    \'clf__n_estimators\': [100, 200, 300, 400, 500],\n",
    "    \'clf__max_depth\': [2, 3, 4, 5, 6, 7, 8],\n",
    "    \'clf__learning_rate\': [0.005, 0.01, 0.05, 0.075, 0.1, 0.2, 0.3],\n",
    "    \'clf__subsample\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'clf__colsample_bytree\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'clf__gamma\': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \'clf__reg_alpha\': [0, 0.01, 0.1, 1, 10],\n",
    "    \'clf__reg_lambda\': [0.01, 0.1, 1, 5, 10]\n",
    "}\n",
    "\n",
    "random_xgb = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_dist,\n",
    "    n_iter=40,              # Número de combinações a testar (aumente para busca mais ampla)\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',      # Pode trocar para \'f1\' se quiser priorizar acerto de positivos\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\'--- XGBoost com RandomizedSearch ---\' )\n",
    "print(\'Melhor ROC-AUC (cv):\', random_xgb.best_score_)\n",
    "print(\'Melhores parâmetros:\', random_xgb.best_params_)\n",
    "\n",
    "best_rnd_xgb = random_xgb.best_estimator_\n",
    "print(f\'Acurácia validação: {best_rnd_xgb.score(X_val, y_val):.3f}\')\n",
    "y_pred_best_rndxgb = best_rnd_xgb.predict(X_val)\n",
    "y_proba_best_rndxgb = best_rnd_xgb.predict_proba(X_val)[:, 1]\n",
    "print(classification_report(y_val, y_pred_best_rndxgb))\n",
    "print(confusion_matrix(y_val, y_pred_best_rndxgb))\n",
    "print(\'ROC-AUC:\', roc_auc_score(y_val, y_proba_best_rndxgb))\n",
    "\n",
    "# Validação cruzada para melhor modelo encontrado\n",
    "best_rnd_xgb_scores = cross_val_score(\n",
    "    best_rnd_xgb, X, y, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "    scoring=\'roc_auc\'\n",
    ")\n",
    "print(f\'Validação cruzada XGBoost (random search) ROC-AUC: {best_rnd_xgb_scores.mean():.3f} ± {best_rnd_xgb_scores.std():.3f}\')\n",
    "\n",
    "\n",
    "# --- Validação cruzada Random Forest ---\n",
    "rf_scores = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\'roc_auc\')\n",
    "print(f\'Validação cruzada RandomForest ROC-AUC: {rf_scores.mean():.3f} ± {rf_scores.std():.3f}\')\n",
    "\n",
    "# --- Validação cruzada XGBoost ---\n",
    "xgb_scores = cross_val_score(xgb_pipeline, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\'roc_auc\')\n",
    "print(f\'Validação cruzada XGBoost ROC-AUC: {xgb_scores.mean():.3f} ± {xgb_scores.std():.3f}\')\n",
    "\n",
    "# --- Validação cruzada XGBoost hiperparametrizado ---\n",
    "best_xgb_scores = cross_val_score(best_xgb, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\'roc_auc\')\n",
    "print(f\'Validação cruzada XGBoost (melhor modelo) ROC-AUC: {best_xgb_scores.mean():.3f} ± {best_xgb_scores.std():.3f}\')\n",
    "\n",
    "\n",
    "# Previsão final para submissão pode ser feita com o melhor modelo\n",
    "# X_test = test.drop([\'id\'], axis=1)\n",
    "# preds = best_rnd_xgb.predict(X_test)\n",
    "# submission = pd.DataFrame({\'id\': test[\'id\'], \'labels\': preds})\n",
    "# submission.to_csv(\'submission.csv\', index=False)\n",
    "\n",
    "\n",
    "# the best public score are from the xgboost default model and xgboost with random search models\n",
    "```"
   ]
  }
]

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4. Abordagem com Target Encoding e Feature Selection (Baseado em `03.py`)\n",
    "\n",
    "Esta iteração explorou o uso de `TargetEncoder` para a variável `category_code` e uma seleção de features baseada na importância de um modelo XGBoost (`SelectFromModel`).\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração com Target Encoding e Feature Selection (03.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "# Para garantir que as variáveis `train` e `test` estejam disponíveis para este bloco de código, \n",
    "# vamos recriá-las aqui, caso o notebook seja executado em partes.\n",
    "# Lembre-se de que, em um notebook real, você carregaria os dados uma única vez no início.\n",
    "try:\n",
    "    train = pd.read_csv(\'train.csv\')\n",
    "    test = pd.read_csv(\'test.csv\')\n",
    "except FileNotFoundError:\n",
    "    print(\"Certifique-se de que \'train.csv\' e \'test.csv\' estão no diretório correto.\")\n",
    "    # Criar DataFrames vazios ou mock para evitar erros se os arquivos não existirem\n",
    "    train = pd.DataFrame(columns=[\"id\", \"labels\"] + [f\"col{i}\" for i in range(30)])\n",
    "    test = pd.DataFrame(columns=[\"id\"] + [f\"col{i}\" for i in range(30)])\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Engenharia de Features (simplificada para este exemplo, focando nas do 01.py)\n",
    "X_te = train.drop([\'id\', \'labels\'], axis=1).copy()\n",
    "y_te = train[\'labels\']\n",
    "X_test_te = test.drop([\'id\'], axis=1).copy()\n",
    "\n",
    "for df in [X_te, X_test_te]:\n",
    "    df[\'funding_per_round\'] = df[\'funding_total_usd\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'milestones_per_year\'] = df[\'milestones\'] / (df[\'age_last_milestone_year\'].fillna(df[\'age_last_milestone_year\'].median()) + 1)\n",
    "    df[\'funding_per_year\'] = df[\'funding_total_usd\'] / (df[\'age_last_funding_year\'].fillna(df[\'age_last_funding_year\'].median()) + 1)\n",
    "    df[\'participants_per_round\'] = df[\'avg_participants\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'relationships_per_round\'] = df[\'relationships\'] / (df[\'funding_rounds\'] + 1e-9)\n",
    "    df[\'is_CA_software\'] = df[\'is_CA\'] * df[\'is_software\']\n",
    "    df[\'is_NY_advertising\'] = df[\'is_NY\'] * df[\'is_advertising\']\n",
    "    df[\'is_MA_biotech\'] = df[\'is_MA\'] * df[\'is_biotech\']\n",
    "    df[\'is_high_funding\'] = (df[\'funding_total_usd\'] > df[\'funding_total_usd\'].median()).astype(int)\n",
    "    df[\'is_high_milestones\'] = (df[\'milestones\'] > df[\'milestones\'].median()).astype(int)\n",
    "    df[\'time_to_first_milestone_after_funding\'] = (\n",
    "        df[\'age_first_milestone_year\'] - df[\'age_first_funding_year\']\n",
    "    )\n",
    "    df[\'milestone_per_million\'] = df[\'milestones\'] / (df[\'funding_total_usd\'] / 1e6 + 1)\n",
    "\n",
    "cont_features_te = [\n",
    "    \'age_first_funding_year\', \'age_last_funding_year\',\n",
    "    \'age_first_milestone_year\', \'age_last_milestone_year\',\n",
    "    \'relationships\', \'funding_rounds\', \'funding_total_usd\',\n",
    "    \'milestones\', \'avg_participants\', \'relationships_per_round\',\n",
    "    \'participants_per_round\', \'funding_per_year\', \'milestones_per_year\',\n",
    "    \'funding_per_round\', \'time_to_first_milestone_after_funding\', \'milestone_per_million\'\n",
    "]\n",
    "bin_features_te = [col for col in X_te.columns if col.startswith(\'is_\') or col.startswith(\'has_\') and col != \'is_othercategory\']\n",
    "cat_features_te = [\'category_code\']\n",
    "\n",
    "all_features_te = cont_features_te + bin_features_te + cat_features_te\n",
    "X_te_processed = X_te[all_features_te].copy()\n",
    "X_test_te_processed = X_test_te[all_features_te].copy()\n",
    "\n",
    "# Target Encoding\n",
    "target_encoder_te = ce.TargetEncoder(cols=[\'category_code\'])\n",
    "X_te_processed[\'category_code\'] = target_encoder_te.fit_transform(X_te_processed[\'category_code\'], y_te)\n",
    "X_test_te_processed[\'category_code\'] = target_encoder_te.transform(X_test_te_processed[\'category_code\'])\n",
    "\n",
    "# Pré-processamento (Imputação e Escalonamento)\n",
    "cont_transformer_te = Pipeline([\n",
    "    (\'imputer\', SimpleImputer(strategy=\'median\')),\n",
    "    (\'scaler\', StandardScaler())\n",
    "]) \n",
    "preprocessor_te = ColumnTransformer([\n",
    "    (\'cont\', cont_transformer_te, cont_features_te),\n",
    "    (\'bin\', \'passthrough\', bin_features_te),\n",
    "    (\'cat\', \'passthrough\', [\'category_code\']) # Passa a coluna já encodada\n",
    "]) \n",
    "\n",
    "X_prep_te = preprocessor_te.fit_transform(X_te_processed)\n",
    "X_test_prep_te = preprocessor_te.transform(X_test_te_processed)\n",
    "\n",
    "# Feature Selection com XGBoost\n",
    "feature_selector_clf_te = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\'logloss\',\n",
    "    scale_pos_weight=(y_te.value_counts()[0]/y_te.value_counts()[1])\n",
    ")\n",
    "feature_selector_clf_te.fit(X_prep_te, y_te)\n",
    "selector_te = SelectFromModel(feature_selector_clf_te, prefit=True, threshold=\'median\')\n",
    "X_fs_te = selector_te.transform(X_prep_te)\n",
    "X_test_fs_te = selector_te.transform(X_test_prep_te)\n",
    "\n",
    "print(f\"\\nNº features selecionadas (Target Encoding + FS): {X_fs_te.shape[1]}/{X_prep_te.shape[1]}\")\n",
    "\n",
    "X_train_split_te, X_val_te, y_train_split_te, y_val_te = train_test_split(X_fs_te, y_te, stratify=y_te, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost com RandomizedSearchCV (após FS)\n",
    "param_dist_te = {\n",
    "    \'n_estimators\': [100, 200, 300, 400, 500],\n",
    "    \'max_depth\': [2, 3, 4, 5, 6, 7],\n",
    "    \'learning_rate\': [0.005, 0.01, 0.05, 0.075, 0.1, 0.2, 0.3],\n",
    "    \'subsample\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'colsample_bytree\': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \'gamma\': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \'reg_alpha\': [0, 0.01, 0.1, 1, 10],\n",
    "    \'reg_lambda\': [0.01, 0.1, 1, 5, 10]\n",
    "}\n",
    "random_xgb_te = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric=\'logloss\',\n",
    "                  scale_pos_weight=(y_train_split_te.value_counts()[0]/y_train_split_te.value_counts()[1])),\n",
    "    param_dist_te,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\'roc_auc\',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "random_xgb_te.fit(X_train_split_te, y_train_split_te)\n",
    "best_xgb_te = random_xgb_te.best_estimator_\n",
    "\n",
    "print(\"\\n===== XGBoost RANDOMIZED SEARCH (Target Encoding + FS) =====\")\n",
    "print(\"Melhores parâmetros:\", random_xgb_te.best_params_)\n",
    "print(f\"Acurácia validação: {best_xgb_te.score(X_val_te, y_val_te):.3f}\")\n",
    "y_pred_val = best_xgb_te.predict(X_val_te)\n",
    "y_proba_val = best_xgb_te.predict_proba(X_val_te)[:,1]\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_val_te, y_pred_val))\n",
    "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(y_val_te, y_pred_val))\n",
    "print(f\'ROC-AUC: {roc_auc_score(y_val_te, y_proba_val):.4f}\' )\n",
    "\n",
    "xgb_cv = cross_val_score(best_xgb_te, X_fs_te, y_te, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=\'roc_auc\')\n",
    "print(f\"Validação cruzada FINAL XGBoost (RandomizedSearchCV) ROC-AUC: {xgb_cv.mean():.4f} ± {xgb_cv.std():.4f}\")\n",
    "\n",
    "# ========== THRESHOLD TUNING ADICIONAL ===========\n",
    "print(\"\\n===== THRESHOLD TUNING (XGBoost) =====\")\n",
    "thresholds = np.arange(0.2, 0.7, 0.005)\n",
    "f1s = []\n",
    "for th in thresholds:\n",
    "    preds = (y_proba_val > th).astype(int)\n",
    "    f1 = f1_score(y_val_te, preds)\n",
    "    f1s.append(f1)\n",
    "best_f1_idx = np.argmax(f1s)\n",
    "best_th = thresholds[best_f1_idx]\n",
    "print(f\"Melhor threshold para submissão: {best_th:.3f} (F1 = {f1s[best_f1_idx]:.4f})\")\n",
    "\n",
    "# ========== SUBMISSÃO ===========\n",
    "# final_proba_test = best_xgb_te.predict_proba(X_test_fs_te)[:,1]\n",
    "# final_preds = (final_proba_test > best_th).astype(int)\n",
    "# submission = pd.DataFrame({\'id\': test[\'id\'], \'labels\': final_preds})\n",
    "# submission.to_csv(\'submission_selected_encoded.csv\', index=False)\n",
    "```"
   ]
  }
]

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5. Abordagem com Feature Engineering e Análise Exploratória (Baseado em `04.py`)\n",
    "\n",
    "Esta iteração focou em uma engenharia de features mais elaborada e em uma análise exploratória detalhada, incluindo visualizações e identificação de multicolinearidade. A variável `category_code` foi removida, e a seleção de features foi manual, baseada nas features engenheiradas.\n",
    "\n",
    "```python\n",
    "# Código adaptado da iteração com Feature Engineering e Análise Exploratória (04.py)\n",
    "# Carregamento dos dados (assumindo já carregados)\n",
    "# train = pd.read_csv(\'train.csv\')\n",
    "# test = pd.read_csv(\'test.csv\')\n",
    "\n",
    "# Para garantir que as variáveis `train` e `test` estejam disponíveis para este bloco de código, \n",
    "# vamos recriá-las aqui, caso o notebook seja executado em partes.\n",
    "# Lembre-se de que, em um notebook real, você carregaria os dados uma única vez no início.\n",
    "try:\n",
    "    train = pd.read_csv(\'train.csv\')\n",
    "    test = pd.read_csv(\'test.csv\')\n",
    "except FileNotFoundError:\n",
    "    print(\"Certifique-se de que \'train.csv\' e \'test.csv\' estão no diretório correto.\")\n",
    "    # Criar DataFrames vazios ou mock para evitar erros se os arquivos não existirem\n",
    "    train = pd.DataFrame(columns=[\"id\", \"labels\"] + [f\"col{i}\" for i in range(30)])\n",
    "    test = pd.DataFrame(columns=[\"id\"] + [f\"col{i}\" for i in range(30)])\n",
    "\n",
    "def engineer_features_04(df):\n",
    "  \"\"\"\n",
    "  Aplica todas as transformações de feature engineering a um dataset\n",
    "  \"\"\"\n",
    "  df_enhanced = df.copy()\n",
    "  \n",
    "  # 1. Features de proporções e ratios\n",
    "  df_enhanced[\'milestones_per_funding_round\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "  df_enhanced[\'avg_funding_per_round\'] = df_enhanced[\'funding_total_usd\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "  df_enhanced[\'relationships_per_milestone\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'milestones\'] + 0.1)\n",
    "  \n",
    "  # 2. Features baseadas em tempo\n",
    "  df_enhanced[\'time_to_first_funding\'] = df_enhanced[\'age_first_funding_year\'].fillna(999)\n",
    "  df_enhanced[\'funding_duration\'] = (df_enhanced[\'age_last_funding_year\'] - df_enhanced[\'age_first_funding_year\']).fillna(0)\n",
    "  df_enhanced[\'time_to_first_milestone\'] = df_enhanced[\'age_first_milestone_year\'].fillna(999)\n",
    "  df_enhanced[\'milestone_duration\'] = (df_enhanced[\'age_last_milestone_year\'] - df_enhanced[\'age_first_milestone_year\']).fillna(0)\n",
    "  \n",
    "  # 3. Features de progressão de funding\n",
    "  df_enhanced[\'funding_progression_score\'] = (\n",
    "      df_enhanced[\'has_roundA\'] * 1 +\n",
    "      df_enhanced[\'has_roundB\'] * 2 +\n",
    "      df_enhanced[\'has_roundC\'] * 3 +\n",
    "      df_enhanced[\'has_roundD\'] * 4\n",
    "  )\n",
    "  df_enhanced[\'funding_diversity\'] = df_enhanced[\'has_VC\'] + df_enhanced[\'has_angel\']\n",
    "  df_enhanced[\'has_later_stage\'] = ((df_enhanced[\'has_roundC\'] == 1) | (df_enhanced[\'has_roundD\'] == 1)).astype(int)\n",
    "  \n",
    "  # 4. Features geográficas\n",
    "  df_enhanced[\'location_success_score\'] = (\n",
    "      df_enhanced[\'is_MA\'] * 0.82 +\n",
    "      df_enhanced[\'is_NY\'] * 0.70 +\n",
    "      df_enhanced[\'is_CA\'] * 0.69 +\n",
    "      df_enhanced[\'is_TX\'] * 0.46 +\n",
    "      df_enhanced[\'is_otherstate\'] * 0.46\n",
    "  )\n",
    "  df_enhanced[\'is_top_tier_location\'] = ((df_enhanced[\'is_CA\'] == 1) | \n",
    "                                        (df_enhanced[\'is_NY\'] == 1) | \n",
    "                                        (df_enhanced[\'is_MA\'] == 1)).astype(int)\n",
    "  \n",
    "  # 5. Features setoriais\n",
    "  df_enhanced[\'sector_success_score\'] = (\n",
    "      df_enhanced[\'is_enterprise\'] * 0.75 +\n",
    "      df_enhanced[\'is_advertising\'] * 0.69 +\n",
    "      df_enhanced[\'is_web\'] * 0.68 +\n",
    "      df_enhanced[\'is_biotech\'] * 0.68 +\n",
    "      df_enhanced[\'is_mobile\'] * 0.66 +\n",
    "      df_enhanced[\'is_software\'] * 0.64 +\n",
    "      df_enhanced[\'is_gamesvideo\'] * 0.62 +\n",
    "      df_enhanced[\'is_othercategory\'] * 0.62 +\n",
    "      df_enhanced[\'is_consulting\'] * 0.50 +\n",
    "      df_enhanced[\'is_ecommerce\'] * 0.40\n",
    "  )\n",
    "  df_enhanced[\'is_high_success_sector\'] = ((df_enhanced[\'is_enterprise\'] == 1) | \n",
    "                                          (df_enhanced[\'is_advertising\'] == 1) | \n",
    "                                          (df_enhanced[\'is_web\'] == 1) | \n",
    "                                          (df_enhanced[\'is_biotech\'] == 1)).astype(int)\n",
    "  \n",
    "  # 6. Features de escala e maturidade\n",
    "  df_enhanced[\'total_activity_score\'] = (\n",
    "      df_enhanced[\'relationships\'] + \n",
    "      df_enhanced[\'milestones\'] + \n",
    "      df_enhanced[\'funding_rounds\']\n",
    "  )\n",
    "  df_enhanced[\'funding_efficiency\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'funding_total_usd\'] / 1000000 + 0.1)\n",
    "  df_enhanced[\'is_mature_startup\'] = ((df_enhanced[\'milestones\'] >= 2) & \n",
    "                                      (df_enhanced[\'funding_rounds\'] >= 2)).astype(int)\n",
    "  \n",
    "  # 7. Features de interação\n",
    "  df_enhanced[\'location_tech_interaction\'] = (\n",
    "      df_enhanced[\'is_top_tier_location\'] * \n",
    "      (df_enhanced[\'is_software\'] + df_enhanced[\'is_web\'] + df_enhanced[\'is_mobile\'])\n",
    "  )\n",
    "  df_enhanced[\'funding_relationships_interaction\'] = (\n",
    "      df_enhanced[\'funding_progression_score\'] * df_enhanced[\'relationships\']\n",
    "  )\n",
    "  df_enhanced[\'maturity_funding_interaction\'] = (\n",
    "      df_enhanced[\'is_mature_startup\'] * np.log1p(df_enhanced[\'funding_total_usd\'])\n",
    "  )\n",
    "  df_enhanced[\'has_recent_milestones\'] = (\n",
    "      (df_enhanced[\'age_last_milestone_year\'].fillna(999) <= 3)\n",
    "  ).astype(int)\n",
    "  \n",
    "  return df_enhanced\n",
    "\n",
    "# Remove colunas não usadas\n",
    "train_04 = train.drop(columns=[\'id\', \'category_code\']) # Assumindo que train é o df original\n",
    "test_04 = test.drop(columns=[\'id\', \'category_code\']) # Assumindo que test é o df original\n",
    "\n",
    "# Feature Engineering\n",
    "X_train_enhanced_04 = engineer_features_04(train_04.drop(columns=[\'labels\']))\n",
    "X_test_enhanced_04 = engineer_features_04(test_04)\n",
    "y_train_04 = train_04[\'labels\']\n",
    "\n",
    "# Tratamento de missing (imputação por mediana)\n",
    "for col in X_train_enhanced_04.columns:\n",
    "    if X_train_enhanced_04[col].dtype in [np.float64, np.int64]:\n",
    "        mediana = X_train_enhanced_04[col].median()\n",
    "        X_train_enhanced_04.fillna({col: mediana}, inplace=True)\n",
    "        X_test_enhanced_04.fillna({col: mediana}, inplace=True)\n",
    "\n",
    "# Tratamento de outliers (tree-based: clipping nos percentis extremos)\n",
    "for col in X_train_enhanced_04.columns:\n",
    "    if X_train_enhanced_04[col].dtype in [np.float64, np.int64]:\n",
    "        lower = X_train_enhanced_04[col].quantile(0.25)\n",
    "        upper = X_train_enhanced_04[col].quantile(0.75)\n",
    "        X_train_enhanced_04[col] = X_train_enhanced_04[col].clip(lower, upper)\n",
    "        X_test_enhanced_04[col] = X_test_enhanced_04[col].clip(lower, upper)\n",
    "\n",
    "# Normalização (opcional para tree-based, mas pode ajudar em análise)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enhanced_04)\n",
    "X_test_scaled = scaler.transform(X_test_enhanced_04)\n",
    "\n",
    "# --- Análise Exploratória ---\n",
    "\n",
    "# Labels\n",
    "print(\"Distribuição do target (labels):\")\n",
    "print(y_train_04.value_counts(normalize=True))\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y_train_04)\n",
    "plt.title(\"Distribuição dos Labels (Sucesso vs Fracasso)\")\n",
    "plt.xlabel(\"Label (0: Fracasso, 1: Sucesso)\")\n",
    "plt.ylabel(\"Quantidade\")\n",
    "plt.show()\n",
    "\n",
    "# Estatísticas\n",
    "print(\"\\nResumo estatístico geral das features:\")\n",
    "print(pd.DataFrame(X_train_enhanced_04).describe().T)\n",
    "\n",
    "# Boxplots das principais features\n",
    "top_features_to_plot = X_train_enhanced_04.columns[:7]\n",
    "for feature in top_features_to_plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=y_train_04, y=X_train_enhanced_04[feature])\n",
    "    plt.title(f\"Boxplot de {feature} por label\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n",
    "\n",
    "# Multicolinearidade\n",
    "corr_matrix = X_train_enhanced_04.corr().abs()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, cmap=\'coolwarm\', annot=False)\n",
    "plt.title(\"Matriz de Correlação entre Features\")\n",
    "plt.show()\n",
    "\n",
    "# Features altamente correlacionadas (> 0.8)\n",
    "high_corr_var = np.where(corr_matrix > 0.8)\n",
    "high_corr_list = [\n",
    "  (X_train_enhanced_04.columns[x], X_train_enhanced_04.columns[y], corr_matrix.iloc[x, y]) \n",
    "  for x, y in zip(*high_corr_var) if x != y and x < y\n",
    "]\n",
    "print(\"\\nPares de features altamente correlacionadas (> 0.8):\")\n",
    "for var1, var2, corr in high_corr_list:\n",
    "  print(f\"{var1} & {var2}: correlação = {corr:.3f}\")\n",
    "\n",
    "# Seleção de features manual\n",
    "engineered_feature_names = [\n",
    "    # Core features (must-have)\n",
    "    \'total_activity_score\',\n",
    "    \'maturity_funding_interaction\', \n",
    "    \'funding_progression_score\',\n",
    "    \'is_mature_startup\',\n",
    "    \'time_to_first_milestone\',\n",
    "    \'funding_relationships_interaction\',\n",
    "    \n",
    "    # Supporting features (high value)\n",
    "    \'milestone_duration\',\n",
    "    \'location_success_score\',\n",
    "    \'funding_duration\',\n",
    "    \'is_top_tier_location\',\n",
    "    \'has_later_stage\',\n",
    "    \n",
    "    # Efficiency features\n",
    "    \'milestones_per_funding_round\',\n",
    "    \'funding_efficiency\',\n",
    "    \'relationships\', # Adicionado para garantir que esteja presente\n",
    "    \'milestones\', # Adicionado para garantir que esteja presente\n",
    "    \'funding_total_usd\' # Adicionado para garantir que esteja presente\n",
    "]\n",
    "\n",
    "X_train_selected_04 = X_train_enhanced_04[engineered_feature_names]\n",
    "X_test_selected_04 = X_test_enhanced_04[engineered_feature_names]\n",
    "\n",
    "# Split para validação (usando stratificação para balanceamento)\n",
    "X_train_split_04, X_val_04, y_train_split_04, y_val_04 = train_test_split(\n",
    "  X_train_selected_04, y_train_04, test_size=0.2, stratify=y_train_04, random_state=42\n",
    ")\n",
    "\n",
    "# Modelagem - exemplo com Logistic Regression e Random Forest\n",
    "models_04 = {\n",
    "  \'RandomForest\': RandomForestClassifier(class_weight=\'balanced\', n_estimators=100, random_state=42),\n",
    "  \'ExtraTrees\': ExtraTreesClassifier(class_weight=\'balanced\', n_estimators=100, random_state=42),\n",
    "  \'XGBClassifier\': XGBClassifier(random_state=42, eval_metric=\'logloss\', n_estimators=100, \n",
    "                                  scale_pos_weight=(y_train_split_04.value_counts()[0]/y_train_split_04.value_counts()[1]))\n",
    "}\n",
    "\n",
    "# === AVALIAÇÃO AVANÇADA ===\n",
    "\n",
    "for name, model_04 in models_04.items():\n",
    "    model_04.fit(X_train_split_04, y_train_split_04)\n",
    "    y_pred_04 = model_04.predict(X_val_04)\n",
    "    y_prob_04 = model_04.predict_proba(X_val_04)[:, 1]\n",
    "    print(f\"\\nModelo: {name}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_val_04, y_prob_04):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_val_04, y_pred_04):.4f}\")\n",
    "    print(classification_report(y_val_04, y_pred_04))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_mat = confusion_matrix(y_val_04, y_pred_04)\n",
    "    print(\"Matriz de Confusão:\\n\", conf_mat)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Matriz de Confusão - {name}\")\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "    # Curva ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_04, y_prob_04)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\'AUC = {roc_auc_score(y_val_04, y_prob_04):.2f}\')\n",
    "    plt.plot([0,1], [0,1], \'k--\')\n",
    "    plt.xlabel(\'False Positive Rate\')\n",
    "    plt.ylabel(\'True Positive Rate\')\n",
    "    plt.title(f\'Curva ROC - {name}\')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Curva Precision-Recall\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val_04, y_prob_04)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\'Recall\')\n",
    "    plt.ylabel(\'Precision\')\n",
    "    plt.title(f\'Curva Precision-Recall - {name}\')\n",
    "    plt.show()\n",
    "\n",
    "# === OTIMIZAÇÃO DE THRESHOLD ===\n",
    "# Exemplo para o último modelo treinado (XGBClassifier neste caso):\n",
    "y_prob_last_model = models_04[\'XGBClassifier\'].predict_proba(X_val_04)[:, 1]\n",
    "thresholds_04 = np.arange(0.3, 0.7, 0.01)\n",
    "f1s_04 = []\n",
    "for th in thresholds_04:\n",
    "    preds_th = (y_prob_last_model > th).astype(int)\n",
    "    f1 = f1_score(y_val_04, preds_th)\n",
    "    f1s_04.append(f1)\n",
    "best_f1_idx_04 = np.argmax(f1s_04)\n",
    "best_th_04 = thresholds_04[best_f1_idx_04]\n",
    "print(f\"Melhor threshold para submissão (04.py): {best_th_04:.3f} (F1 = {f1s_04[best_f1_idx_04]:.4f})\")\n",
    "\n",
    "# === INTERPRETAÇÃO DOS RESULTADOS ===\n",
    "# (Exemplo com Feature Importance tradicional para RandomForest)\n",
    "# Note: SHAP pode ser computacionalmente intensivo para grandes datasets/modelos\n",
    "# explainer = shap.TreeExplainer(models_04[\'RandomForest\'])\n",
    "# shap_values = explainer.shap_values(X_val_04)\n",
    "# shap.summary_plot(shap_values, X_val_04, feature_names=X_val_04.columns)\n",
    "\n",
    "importances = models_04[\'RandomForest\'].feature_importances_\n",
    "feature_importance_df_04 = pd.DataFrame({\n",
    "    \'feature\': X_val_04.columns,\n",
    "    \'importance\': importances\n",
    "}).sort_values(\'importance\', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(x=\'importance\', y=\'feature\', data=feature_importance_df_04.head(10))\n",
    "plt.title(\"Top 10 Feature Importances (RandomForest - 04.py)\")\n",
    "plt.show()\n",
    "\n",
    "# === PREVISÃO NO TEST SET ===\n",
    "# test_pred_prob = models_04[\'XGBClassifier\'].predict_proba(X_test_selected_04)[:, 1]\n",
    "# test_pred_label = (test_pred_prob >= best_th_04).astype(int)\n",
    "#\n",
    "# sample_submission = pd.DataFrame({\'id\': test[\'id\'], \'labels\': test_pred_label})\n",
    "# sample_submission.to_csv(\'submission_04.csv\', index=False)\n",
    "```"
   ]
  }
]

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Execução do Pipeline Principal\n",
    "\n",
    "Aqui, o pipeline completo é executado, aplicando todas as etapas de engenharia de features, pré-processamento, balanceamento de classes, seleção de features e modelagem em *ensemble*.\n",
    "\n",
    "```python\n",
    "# Carregamento dos dados (garantindo que estejam disponíveis)\n",
    "try:\n",
    "    train = pd.read_csv(\'train.csv\')\n",
    "    test = pd.read_csv(\'test.csv\')\n",
    "except FileNotFoundError:\n",
    "    print(\"Certifique-se de que \'train.csv\' e \'test.csv\' estão no diretório correto.\")\n",
    "    # Criar DataFrames vazios ou mock para evitar erros se os arquivos não existirem\n",
    "    train = pd.DataFrame(columns=[\"id\", \"labels\"] + [f\"col{i}\" for i in range(30)])\n",
    "    test = pd.DataFrame(columns=[\"id\"] + [f\"col{i}\" for i in range(30)])\n",
    "\n",
    "# 1. Engenharia de Features\n",
    "def engineer_features_improved(df):\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. FEATURES ORIGINAIS MANTIDAS (bem alinhadas com literatura)\n",
    "    df_enhanced[\'milestones_per_funding_round\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "    df_enhanced[\'avg_funding_per_round\'] = df_enhanced[\'funding_total_usd\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "    df_enhanced[\'relationships_per_milestone\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'milestones\'] + 0.1)\n",
    "    \n",
    "    df_enhanced[\'time_to_first_funding\'] = df_enhanced[\'age_first_funding_year\'].fillna(999)\n",
    "    df_enhanced[\'funding_duration\'] = (df_enhanced[\'age_last_funding_year\'] - df_enhanced[\'age_first_funding_year\']).fillna(0)\n",
    "    df_enhanced[\'time_to_first_milestone\'] = df_enhanced[\'age_first_milestone_year\'].fillna(999)\n",
    "    df_enhanced[\'milestone_duration\'] = (df_enhanced[\'age_last_milestone_year\'] - df_enhanced[\'age_first_milestone_year\']).fillna(0)\n",
    "    \n",
    "    df_enhanced[\'funding_progression_score\'] = (\n",
    "        df_enhanced[\'has_roundA\'] * 1 +\n",
    "        df_enhanced[\'has_roundB\'] * 2 + \n",
    "        df_enhanced[\'has_roundC\'] * 3 +\n",
    "        df_enhanced[\'has_roundD\'] * 4\n",
    "    )\n",
    "    df_enhanced[\'funding_diversity\'] = df_enhanced[\'has_VC\'] + df_enhanced[\'has_angel\']\n",
    "    df_enhanced[\'has_later_stage\'] = ((df_enhanced[\'has_roundC\'] == 1) | (df_enhanced[\'has_roundD\'] == 1)).astype(int)\n",
    "    \n",
    "    # 2. MELHORIAS NAS FEATURES GEOGRÁFICAS E SETORIAIS\n",
    "    df_enhanced[\'is_innovation_hub\'] = (\n",
    "        (df_enhanced[\'is_CA\'] == 1) | \n",
    "        (df_enhanced[\'is_NY\'] == 1) | \n",
    "        (df_enhanced[\'is_MA\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'is_secondary_hub\'] = (df_enhanced[\'is_TX\'] == 1).astype(int)\n",
    "    \n",
    "    df_enhanced[\'is_tech_sector\'] = (\n",
    "        (df_enhanced[\'is_software\'] == 1) |\n",
    "        (df_enhanced[\'is_web\'] == 1) |\n",
    "        (df_enhanced[\'is_mobile\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'is_high_capital_sector\'] = (\n",
    "        (df_enhanced[\'is_biotech\'] == 1) |\n",
    "        (df_enhanced[\'is_enterprise\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'is_consumer_facing\'] = (\n",
    "        (df_enhanced[\'is_ecommerce\'] == 1) |\n",
    "        (df_enhanced[\'is_advertising\'] == 1) |\n",
    "        (df_enhanced[\'is_gamesvideo\'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 3. NOVAS FEATURES BASEADAS NA LITERATURA\n",
    "    df_enhanced[\'network_density\'] = df_enhanced[\'relationships\'] / (df_enhanced[\'funding_rounds\'] + 1)\n",
    "    df_enhanced[\'relationship_efficiency\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'relationships\'] + 0.1)\n",
    "    df_enhanced[\'team_network_strength\'] = df_enhanced[\'relationships\'] * df_enhanced[\'milestones\'] / 10\n",
    "    df_enhanced[\'founder_experience_proxy\'] = np.minimum(df_enhanced[\'relationships\'] / 5, 3)\n",
    "    df_enhanced[\'team_size_proxy\'] = np.log1p(df_enhanced[\'relationships\']) / 2\n",
    "    \n",
    "    df_enhanced[\'capital_efficiency\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'funding_total_usd\'] / 1000000 + 0.1)\n",
    "    df_enhanced[\'funding_momentum\'] = df_enhanced[\'funding_rounds\'] / (df_enhanced[\'age_last_funding_year\'] + 0.1)\n",
    "    df_enhanced[\'funding_velocity\'] = df_enhanced[\'funding_total_usd\'] / (df_enhanced[\'age_last_funding_year\'] + 0.1)\n",
    "    df_enhanced[\'burn_rate_proxy\'] = df_enhanced[\'funding_total_usd\'] / (df_enhanced[\'funding_duration\'] + 0.1)\n",
    "    df_enhanced[\'runway_efficiency\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'burn_rate_proxy\'] + 0.1)\n",
    "    \n",
    "    df_enhanced[\'traction_proxy\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'time_to_first_funding\'] + 1)\n",
    "    df_enhanced[\'development_speed\'] = df_enhanced[\'milestones\'] / (df_enhanced[\'funding_rounds\'] + 0.1)\n",
    "    df_enhanced[\'market_validation_score\'] = (\n",
    "        (df_enhanced[\'milestones\'] > 0).astype(int) +\n",
    "        (df_enhanced[\'funding_rounds\'] > 1).astype(int) +\n",
    "        (df_enhanced[\'has_VC\'] == 1).astype(int)\n",
    "    )\n",
    "    df_enhanced[\'customer_traction_proxy\'] = df_enhanced[\'relationships\'] * df_enhanced[\'milestones\'] / (df_enhanced[\'funding_rounds\'] + 1)\n",
    "    df_enhanced[\'product_maturity\'] = np.minimum(df_enhanced[\'milestone_duration\'] / 2, 5)\n",
    "    \n",
    "    df_enhanced[\'funding_concentration_risk\'] = df_enhanced[\'avg_funding_per_round\'] / (df_enhanced[\'funding_total_usd\'] + 0.1)\n",
    "    df_enhanced[\'milestone_stagnation_risk\'] = (df_enhanced[\'age_last_milestone_year\'].fillna(999) > 3).astype(int)\n",
    "    df_enhanced[\'late_funding_risk\'] = (df_enhanced[\'time_to_first_funding\'] > 2).astype(int)\n",
    "    df_enhanced[\'funding_gap_risk\'] = np.maximum(0, df_enhanced[\'age_last_funding_year\'] - 2)\n",
    "    df_enhanced[\'low_activity_risk\'] = (\n",
    "        (df_enhanced[\'milestones\'] == 0) | \n",
    "        (df_enhanced[\'relationships\'] < 3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'startup_maturity_score\'] = (\n",
    "        (df_enhanced[\'milestones\'] >= 1).astype(int) +\n",
    "        (df_enhanced[\'funding_rounds\'] >= 2).astype(int) +\n",
    "        (df_enhanced[\'relationships\'] >= 5).astype(int) +\n",
    "        (df_enhanced[\'has_later_stage\'] == 1).astype(int) * 2\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'scale_readiness\'] = (\n",
    "        (df_enhanced[\'funding_progression_score\'] >= 2) &\n",
    "        (df_enhanced[\'milestones\'] >= 2) &\n",
    "        (df_enhanced[\'relationships\'] >= 3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'growth_stage_indicator\'] = np.minimum(\n",
    "        df_enhanced[\'funding_progression_score\'] + df_enhanced[\'startup_maturity_score\'], 8\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'optimal_funding_timing\'] = (\n",
    "        (df_enhanced[\'time_to_first_funding\'] >= 0.5) & \n",
    "        (df_enhanced[\'time_to_first_funding\'] <= 2.0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_enhanced[\'funding_consistency\'] = 1 / (df_enhanced[\'funding_duration\'] + 0.1)\n",
    "    df_enhanced[\'competitive_positioning\'] = (\n",
    "        df_enhanced[\'is_innovation_hub\'].astype(int) * 2 +\n",
    "        df_enhanced[\'is_tech_sector\'].astype(int) +\n",
    "        (df_enhanced[\'funding_total_usd\'] > df_enhanced[\'funding_total_usd\'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'location_funding_synergy\'] = (\n",
    "        df_enhanced[\'is_innovation_hub\'] * np.log1p(df_enhanced[\'funding_total_usd\'])\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'sector_funding_fit\'] = (\n",
    "        df_enhanced[\'is_high_capital_sector\'] * df_enhanced[\'funding_progression_score\']\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'team_market_fit\'] = df_enhanced[\'network_density\'] * df_enhanced[\'market_validation_score\']\n",
    "    df_enhanced[\'execution_capability\'] = df_enhanced[\'development_speed\'] * df_enhanced[\'capital_efficiency\']\n",
    "    \n",
    "    df_enhanced[\'overall_health_score\'] = (\n",
    "        df_enhanced[\'startup_maturity_score\'] * 0.3 +\n",
    "        df_enhanced[\'market_validation_score\'] * 0.4 +\n",
    "        df_enhanced[\'competitive_positioning\'] * 0.3\n",
    "    )\n",
    "    \n",
    "    df_enhanced[\'investment_attractiveness\'] = (\n",
    "        df_enhanced[\'funding_progression_score\'] * 0.4 +\n",
    "        df_enhanced[\'capital_efficiency\'] * 0.3 +\n",
    "        df_enhanced[\'network_density\'] * 0.3\n",
    "    )\n",
    "\n",
    "    df_enhanced[\'funding_recency\'] = 10 - df_enhanced[\'age_last_funding_year\'].fillna(10)\n",
    "    df_enhanced[\'milestone_recency\'] = 10 - df_enhanced[\'age_last_milestone_year\'].fillna(10)\n",
    "    df_enhanced[\'activity_recency_score\'] = df_enhanced[\'funding_recency\'] + df_enhanced[\'milestone_recency\']\n",
    "\n",
    "    df_enhanced[\'high_quality_network\'] = (df_enhanced[\'relationships\'] > df_enhanced[\'relationships\'].quantile(0.75)).astype(int)\n",
    "    df_enhanced[\'rapid_development\'] = (df_enhanced[\'development_speed\'] > df_enhanced[\'development_speed\'].quantile(0.75)).astype(int) # Adicionado\n",
    "    df_enhanced[\'is_well_funded\'] = (df_enhanced[\'funding_total_usd\'] / 1000000 > (df_enhanced[\'funding_total_usd\'] / 1000000).quantile(0.75)).astype(int) # Adicionado\n",
    "    df_enhanced[\'success_signals\'] = df_enhanced[\'has_later_stage\'] + df_enhanced[\'is_innovation_hub\'] + df_enhanced[\'high_quality_network\'] + df_enhanced[\'rapid_development\'] + df_enhanced[\'is_well_funded\']\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# 2. Pré-processamento avançado (tratamento de missing e outliers)\n",
    "def advanced_preprocessing_pipeline(X_train, y_train, X_test):\n",
    "    print(\"=== INICIANDO PIPELINE AVANÇADO ===\")\n",
    "    print(\"1. Tratamento de missing values...\")\n",
    "    \n",
    "    temporal_features = [\'age_first_funding_year\', \'age_last_funding_year\', \n",
    "                        \'age_first_milestone_year\', \'age_last_milestone_year\']\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        if col in temporal_features:\n",
    "            # Para temporais, manter NaN como valor alto (evento não ocorreu) - já tratado na FE\n",
    "            continue\n",
    "        elif X_train[col].dtype in [np.float64, np.int64]:\n",
    "            # Para outras numéricas, usar mediana\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    print(\"2. Tratamento de outliers...\")\n",
    "    \n",
    "    preserve_outliers = [\'funding_total_usd\', \'relationships\', \'milestones\']\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].dtype in [np.float64, np.int64] and col not in preserve_outliers:\n",
    "            Q1 = X_train[col].quantile(0.05)\n",
    "            Q3 = X_train[col].quantile(0.95)\n",
    "            X_train[col] = X_train[col].clip(Q1, Q3)\n",
    "            X_test[col] = X_test[col].clip(Q1, Q3)\n",
    "    \n",
    "    print(\"3. Pipeline de preprocessing concluído!\")\n",
    "    return X_train, X_test\n",
    "\n",
    "# 3. Criação do pipeline de ML\n",
    "def create_advanced_ml_pipeline():\n",
    "    feature_selector = RFECV(\n",
    "        estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        step=1,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring=\'roc_auc\',\n",
    "        min_features_to_select=15,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    models = {\n",
    "        \'rf\': RandomForestClassifier(\n",
    "            n_estimators=150, max_depth=12, min_samples_split=5, min_samples_leaf=2,\n",
    "            class_weight=\'balanced\', random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \'xgb\': XGBClassifier(\n",
    "            n_estimators=150, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \'lgb\': LGBMClassifier(\n",
    "            n_estimators=150, max_depth=6, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8,\n",
    "            class_weight=\'balanced\', random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=list(models.items()),\n",
    "        voting=\'soft\',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "      (\'scaler\', scaler),\n",
    "      (\'imputer\', SimpleImputer(strategy=\'median\')), # Fallback imputer\n",
    "      (\'feature_selection\', feature_selector),\n",
    "      (\'smote\', smote),\n",
    "      (\'ensemble\', ensemble)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# 4. Avaliação abrangente do modelo\n",
    "def comprehensive_model_evaluation(pipeline, X_train, y_train, X_val, y_val):\n",
    "    print(\"\\n=== AVALIAÇÃO ABRANGENTE DO MODELO ===\")\n",
    "    print(\"Treinando pipeline completo...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    y_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_val, y_prob)\n",
    "    auc_pr = average_precision_score(y_val, y_prob)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS PRINCIPAIS:\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"AUC-PR: {auc_pr:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\nRELATÓRIO DETALHADO:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "    print(f\"\\nMATRIZ DE CONFUSÃO:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_val, y_prob)\n",
    "    f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(f\"\\nOTIMIZAÇÃO DE THRESHOLD:\")\n",
    "    print(f\"Threshold ótimo: {optimal_threshold:.3f}\")\n",
    "    print(f\"F1-Score ótimo: {f1_scores[optimal_idx]:.4f}\")\n",
    "    \n",
    "    return pipeline, optimal_threshold, {\n",
    "        \'auc_roc\': auc_roc,\n",
    "        \'auc_pr\': auc_pr,\n",
    "        \'f1\': f1,\n",
    "        \'precision\': precision,\n",
    "        \'recall\': recall,\n",
    "        \'optimal_threshold\': optimal_threshold\n",
    "    }\n",
    "\n",
    "# 5. Análise de importância das features\n",
    "def feature_importance_analysis(pipeline, feature_names):\n",
    "    print(\"\\n=== ANÁLISE DE IMPORTÂNCIA DAS FEATURES ===\")\n",
    "    try:\n",
    "        # O RFECV já selecionou as features, então precisamos obter os nomes das features selecionadas\n",
    "        # e então obter as importâncias do modelo final (ensemble)\n",
    "        selector = pipeline.named_steps[\'feature_selection\']\n",
    "        selected_feature_names = [feature_names[i] for i, support in enumerate(selector.support_) if support]\n",
    "        \n",
    "        ensemble = pipeline.named_steps[\'ensemble\']\n",
    "        importances = np.zeros(len(selected_feature_names))\n",
    "        \n",
    "        for name, model in ensemble.estimators_:\n",
    "            if hasattr(model, \'feature_importances_\'):\n",
    "                importances += model.feature_importances_\n",
    "        \n",
    "        importances /= len(ensemble.estimators_)\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            \'feature\': selected_feature_names,\n",
    "            \'importance\': importances\n",
    "        }).sort_values(\'importance\', ascending=False)\n",
    "        \n",
    "        print(\"TOP 10 Features Mais Importantes:\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.barplot(x=\'importance\', y=\'feature\', data=feature_importance_df.head(10))\n",
    "        plt.title(\'Top 10 Feature Importances (Ensemble)\' )\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao calcular feature importances: {e}\")\n",
    "\n",
    "# Preparação dos dados para o pipeline principal\n",
    "X = train.drop([\'id\', \'labels\'], axis=1)\n",
    "y = train[\'labels\']\n",
    "X_test_final = test.drop([\'id\'], axis=1)\n",
    "\n",
    "# Aplicar engenharia de features\n",
    "X_engineered = engineer_features_improved(X.copy())\n",
    "X_test_engineered = engineer_features_improved(X_test_final.copy())\n",
    "\n",
    "# Alinhar colunas após engenharia de features\n",
    "common_cols = list(set(X_engineered.columns) & set(X_test_engineered.columns))\n",
    "X_engineered = X_engineered[common_cols]\n",
    "X_test_engineered = X_test_engineered[common_cols]\n",
    "\n",
    "# Separar dados em treino e validação\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_engineered, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar pré-processamento avançado (missing e outliers)\n",
    "X_train_processed, X_val_processed = advanced_preprocessing_pipeline(X_train_split.copy(), y_train_split, X_val_split.copy())\n",
    "X_train_processed, X_test_processed = advanced_preprocessing_pipeline(X_train_split.copy(), y_train_split, X_test_engineered.copy())\n",
    "\n",
    "# Criar e avaliar o pipeline de ML\n",
    "final_pipeline = create_advanced_ml_pipeline()\n",
    "final_pipeline, optimal_threshold, metrics = comprehensive_model_evaluation(\n",
    "    final_pipeline, X_train_processed, y_train_split, X_val_processed, y_val_split\n",
    ")\n",
    "\n",
    "# Análise de importância das features\n",
    "feature_importance_analysis(final_pipeline, X_train_processed.columns.tolist())\n",
    "\n",
    "# Geração do arquivo de submissão\n",
    "final_proba_test = final_pipeline.predict_proba(X_test_processed)[:, 1]\n",
    "final_preds = (final_proba_test > optimal_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\'id\': test[\'id\'], \'labels\': final_preds})\n",
    "submission.to_csv(\'submission.csv\', index=False)\n",
    "print(\"Arquivo de submissão gerado: submission.csv\")\n",
    "```"
   ]
  }
]
